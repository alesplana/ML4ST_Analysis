{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "# Paths\n",
    "data_dir = '../data/jp_morgan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# aml_data = pd.read_csv(f'{data_dir}/aml_syn_data.csv')\n",
    "fraud_data = pd.read_csv(f'{data_dir}/fraud_payment_data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_data['Old_Txn_Type'] = fraud_data['Transaction_Type']\n",
    "fraud_data['Transaction_Type'] = fraud_data['Transaction_Id'].str.split('-').str[:-1].str.join('-')\n",
    "fraud_data['Transaction_Type'] = fraud_data['Old_Txn_Type'] + '-' + fraud_data['Transaction_Type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_data['Hybrid_Txn_Type'] = fraud_data['Old_Txn_Type'] + '-' + fraud_data['Transaction_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_data['Hybrid_Txn_Type'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = [\n",
    "    'Time_step',\n",
    "    'Label',\n",
    "    'Transaction_Id',\n",
    "    'Transaction_Type',\n",
    "    'Sender_Id',\n",
    "    'Sender_Account',\n",
    "    'Sender_Country',\n",
    "    'Bene_Id', \n",
    "    'Bene_Account',\n",
    "    'Bene_Country',\n",
    "    'USD_amount',\n",
    "]\n",
    "# aml_data = aml_data[common_columns]\n",
    "fraud_data = fraud_data[common_columns]\n",
    "aml_data = fraud_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "df = pl.DataFrame(aml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds_mapping import fraud\n",
    "txn_mapping = fraud.txn_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_map_transactions(df: pl.DataFrame, txn_mapping: dict) -> pl.DataFrame:\n",
    "    # Pre-compute the mappings\n",
    "    type_mapping = {k: v['std_txn_type'] for k, v in txn_mapping.items()}\n",
    "    method_mapping = {k: v['std_txn_method'] for k, v in txn_mapping.items()}\n",
    "    \n",
    "    # First check for unmapped values\n",
    "    unique_txn_types = df.get_column('Transaction_Type').unique().to_list()  # Note the column name change\n",
    "    unmapped_txns = [txn for txn in unique_txn_types if txn not in type_mapping]\n",
    "    \n",
    "    if unmapped_txns:\n",
    "        raise ValueError(\n",
    "            f\"Found {len(unmapped_txns)} unmapped transaction types:\\n\"\n",
    "            f\"{unmapped_txns}\\n\"\n",
    "            f\"Please update the mapping dictionary with these values.\"\n",
    "        )\n",
    "    \n",
    "    # If we get here, all values are mapped, so proceed with mapping\n",
    "    return df.with_columns([\n",
    "        pl.col('Transaction_Type').replace(type_mapping).alias('std_txn_type'),\n",
    "        pl.col('Transaction_Type').replace(method_mapping).alias('std_txn_method')\n",
    "    ])\n",
    "\n",
    "df = std_map_transactions(df, txn_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Columns:\n",
    "* hour of transaction\n",
    "* normalised age on transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "1. Filter only transactions considering money movements\n",
    "2. Retable DataFrame to be in Party v Counterparty format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AML Data Preprocessing\n",
    "# Filter only money movements\n",
    "# aml_data = aml_data[aml_data['Transaction_Type'].isin(['WIRE', 'PAYMENT', 'WITHDRAWL', 'CASH-DEPOSIT', 'CRYPTO-TRANSFER'])].reset_index()\n",
    "\n",
    "# Identify if row is reversed for party v counterparty retabling\n",
    "def pd_party_reverse_flag(\n",
    "        df: pd.DataFrame, \n",
    "        sender_col: str = 'Sender_Id', \n",
    "        bene_col: str = 'Bene_Id', \n",
    "        transaction_col: str = 'Transaction_Type'\n",
    "        ) -> pd.DataFrame:\n",
    "\n",
    "    df.loc[:, 'sender_type'] = df[sender_col].str.split('-').str[:2].str.join('-')\n",
    "    df.loc[:, 'benef_type'] = df[bene_col].str.split('-').str[:2].str.join('-')\n",
    "    condition = (\n",
    "        (df[transaction_col] == 'CASH-DEPOSIT') |\n",
    "        (((df[transaction_col] == 'MAKE-PAYMENT') | (df[transaction_col] == 'QUICK-PAYMENT') | (df[transaction_col] == 'MOVE-FUNDS')) & (df['benef_type'] == 'JPMC-CLIENT'))\n",
    "    )\n",
    "    df.loc[:, 'is_credit'] = condition\n",
    "    df.loc[:, 'reversed'] = condition\n",
    "    return df\n",
    "\n",
    "def pl_party_reverse_flag(\n",
    "        df: pl.DataFrame, \n",
    "        sender_col: str = 'Sender_Id', \n",
    "        bene_col: str = 'Bene_Id', \n",
    "        transaction_col: str = 'Transaction_Type'\n",
    "        ) -> pl.DataFrame:\n",
    "\n",
    "    df = df.with_columns([\n",
    "        pl.col(sender_col).str.split('-').list.slice(0, 2).list.join('-').alias('sender_type'),\n",
    "        pl.col(bene_col).str.split('-').list.slice(0, 2).list.join('-').alias('benef_type')\n",
    "    ])\n",
    "    \n",
    "    condition = (\n",
    "        (pl.col(transaction_col) == 'DEPOSIT') |\n",
    "        (((pl.col(transaction_col) == 'PAYMENT') | \n",
    "          (pl.col(transaction_col) == 'TRANSFER')) & \n",
    "         (pl.col('benef_type') == 'JPMC-CLIENT'))\n",
    "    )\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        condition.alias('is_credit'),\n",
    "        condition.alias('reversed')\n",
    "    ])\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = pl_party_reverse_flag(df, transaction_col='std_txn_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create party v counterparty retabling\n",
    "def pd_new_table(\n",
    "        df: pd.DataFrame, \n",
    "        rev_col: str = 'reversed', \n",
    "        retained_cols: Optional[List[str]] = None\n",
    "        ) -> pd.DataFrame:\n",
    "\n",
    "    for type in ('Id', 'Account', 'Country'):\n",
    "        df.loc[:, f'party_{type}'] = df[f'Sender_{type}'].where(~df[rev_col], df[f'Bene_{type}'])\n",
    "        df.loc[:, f'cparty_{type}'] = df[f'Bene_{type}'].where(~df[rev_col], df[f'Sender_{type}'])\n",
    "    retained_cols.extend(['party_Id', 'party_Account', 'party_Country',\n",
    "                          'cparty_Id', 'cparty_Account', 'cparty_Country'])\n",
    "    new_table = df[retained_cols]\n",
    "    return new_table\n",
    "\n",
    "def pl_new_table(\n",
    "        df: pl.DataFrame, \n",
    "        rev_col: str = 'reversed', \n",
    "        retained_cols: Optional[List[str]] = None\n",
    "        ) -> pl.DataFrame:\n",
    "\n",
    "    if retained_cols is None:\n",
    "        retained_cols = []\n",
    "\n",
    "    for type in ('Id', 'Account', 'Country'):\n",
    "        df = df.with_columns([\n",
    "            pl.when(~pl.col(rev_col)).then(pl.col(f'Sender_{type}')).otherwise(pl.col(f'Bene_{type}')).alias(f'party_{type}'),\n",
    "            pl.when(~pl.col(rev_col)).then(pl.col(f'Bene_{type}')).otherwise(pl.col(f'Sender_{type}')).alias(f'cparty_{type}')\n",
    "        ])\n",
    "    \n",
    "    retained_cols.extend(['party_Id', 'party_Account', 'party_Country',\n",
    "                          'cparty_Id', 'cparty_Account', 'cparty_Country'])\n",
    "    \n",
    "    new_table = df.select(retained_cols)\n",
    "    return new_table\n",
    "\n",
    "\n",
    "df = pl_new_table(df, retained_cols=[\n",
    "    'Time_step', \n",
    "    'Label',\n",
    "    'Transaction_Id',\n",
    "    'Transaction_Type',\n",
    "    'std_txn_type',\n",
    "    'std_txn_method',\n",
    "    'is_credit',\n",
    "    'USD_amount'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Normalise dates (change into age of account on transaction) and extract hour/minute for off hours transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise date and extract hour minutes\n",
    "def pd_feature_time(\n",
    "        df: pd.DataFrame, \n",
    "        dt_column: str = 'Time_step'\n",
    "        ) -> pd.DataFrame:\n",
    "    \n",
    "    time_series = pd.to_datetime(df[dt_column])\n",
    "    df['txn_time_hr'] = time_series.dt.hour\n",
    "    df['txn_time_mm'] = time_series.dt.minute\n",
    "    \n",
    "    return df\n",
    "\n",
    "def pd_normalise_date(df, dt_column='Time_step', primary_col='party_Id'):\n",
    "    time_series = pd.to_datetime(df[dt_column]).dt.date\n",
    "    df['txn_age_days'] = (time_series - time_series.groupby(df[primary_col]).transform('min')).astype('timedelta64[ns]').dt.days\n",
    "    \n",
    "    return df\n",
    "\n",
    "def pl_feature_time(\n",
    "        df: pl.DataFrame, \n",
    "        dt_column: str = 'Time_step'\n",
    "        ) -> pl.DataFrame:\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        pl.col(dt_column).str.strptime(pl.Datetime).alias('parsed_time')\n",
    "    ])\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        pl.col('parsed_time').dt.hour().alias('txn_time_hr'),\n",
    "        pl.col('parsed_time').dt.minute().alias('txn_time_mm')\n",
    "    ])\n",
    "    \n",
    "    return df.drop('parsed_time')\n",
    "\n",
    "def pl_normalise_date(\n",
    "        df: pl.DataFrame, \n",
    "        dt_column: str = 'Time_step', \n",
    "        primary_col: str = 'party_Id',\n",
    "        dt_fmt: str = '%Y-%m-%d %H:%M:%S'\n",
    "        ) -> pl.DataFrame:\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        pl.col(dt_column).str.strptime(pl.Date, format=dt_fmt).alias('parsed_date')\n",
    "    ])\n",
    "    \n",
    "    min_dates = df.group_by(primary_col).agg(pl.col('parsed_date').min().alias('min_date'))\n",
    "    df = df.join(min_dates, on=primary_col)\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        (pl.col('parsed_date') - pl.col('min_date')).dt.total_days().alias('txn_age_days')\n",
    "    ])\n",
    "    \n",
    "    return df.drop(['parsed_date', 'min_date'])\n",
    "\n",
    "df = pl_feature_time(df)\n",
    "df = pl_normalise_date(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Collect previous transaction details: (transaction type, amount, if applicable delta between transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract previous transaction details\n",
    "dt_norm_values = {\n",
    "    'day': 'txn_age_days',\n",
    "    'hour': 'txn_time_hr',\n",
    "    'min': 'txn_time_mm',\n",
    "}\n",
    "def pd_fill_prev_txn(\n",
    "        df: pd.DataFrame, \n",
    "        primary_key: str = 'party_Id', \n",
    "        dt_column: str | None = None,\n",
    "        dt_norm_columns: Dict[str, str] | None = None, \n",
    "        columns_to_fill: List[str] = [], \n",
    "        shift: int = 1, \n",
    "        include_age_delta: bool = False\n",
    "        ) -> pd.DataFrame:\n",
    "    \n",
    "    if dt_column != None:\n",
    "        core_columns = [primary_key, dt_column]\n",
    "        temp_df = df[core_columns + columns_to_fill].copy()\n",
    "\n",
    "        temp_df[dt_column] = pd.to_datetime(temp_df[dt_column])\n",
    "        temp_df = temp_df.sort_values([primary_key, dt_column])\n",
    "        \n",
    "        temp_df['prev_age_delta'] = (\n",
    "            (temp_df[dt_column] - temp_df.groupby(primary_key)[dt_column].shift(shift))\n",
    "            .dt.total_seconds())/(24 * 3600)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        dt_norm_cols = [dt_norm_columns['day'], dt_norm_columns['hour'], dt_norm_columns['min']]\n",
    "        core_columns = [primary_key] + dt_norm_cols\n",
    "        temp_df = df[core_columns + columns_to_fill].copy()\n",
    "        \n",
    "        temp_df = temp_df.sort_values(by=[primary_key] + dt_norm_cols, ascending=True)\n",
    "        temp_df['rel_age'] = (\n",
    "            temp_df[dt_norm_columns['day']] + \n",
    "            (temp_df[dt_norm_columns['hour']] / 24) + \n",
    "            (temp_df[dt_norm_columns['min']] / (24 * 60))\n",
    "            )\n",
    "        temp_df['prev_age_delta'] = temp_df['rel_age'] - temp_df.groupby(primary_key)['rel_age'].shift(shift)\n",
    "    \n",
    "    new_columns = ['prev_' + col for col in columns_to_fill]\n",
    "\n",
    "    temp_df[new_columns] = temp_df.groupby(primary_key)[columns_to_fill].shift(shift)\n",
    "    if include_age_delta:\n",
    "        new_columns.append('prev_age_delta')\n",
    "    \n",
    "    return df.merge(temp_df[new_columns], left_index=True, right_index=True)\n",
    "\n",
    "def pl_fill_prev_txn(\n",
    "        df: pl.DataFrame, \n",
    "        primary_key: str = 'party_Id', \n",
    "        dt_column: str | None = None,\n",
    "        dt_norm_columns: Dict[str, str] | None = None, \n",
    "        columns_to_fill: List[str] = [], \n",
    "        shift: int = 1, \n",
    "        include_age_delta: bool = False\n",
    "        ) -> pl.DataFrame:\n",
    "    \n",
    "    if dt_column is not None:\n",
    "        core_columns = [primary_key, dt_column]\n",
    "        temp_df = df.select(core_columns  + columns_to_fill)\n",
    "\n",
    "        temp_df = temp_df.with_columns([\n",
    "            pl.col(dt_column).str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(dt_column)\n",
    "        ])\n",
    "        temp_df = temp_df.sort([primary_key, dt_column])\n",
    "        \n",
    "        temp_df = temp_df.with_columns([\n",
    "            ((pl.col(dt_column) - pl.col(dt_column).shift(shift)).dt.seconds() / (24 * 3600)).alias('prev_age_delta')\n",
    "        ])\n",
    "        \n",
    "    else:\n",
    "        dt_norm_cols = [dt_norm_columns['day'], dt_norm_columns['hour'], dt_norm_columns['min']]\n",
    "        core_columns = [primary_key] + dt_norm_cols\n",
    "        temp_df = df.select(core_columns + columns_to_fill)\n",
    "        \n",
    "        temp_df = temp_df.sort([primary_key] + dt_norm_cols)\n",
    "        temp_df = temp_df.with_columns([\n",
    "            (pl.col(dt_norm_columns['day']) + \n",
    "             (pl.col(dt_norm_columns['hour']) / 24) + \n",
    "             (pl.col(dt_norm_columns['min']) / (24 * 60))).alias('rel_age')\n",
    "        ])\n",
    "        temp_df = temp_df.with_columns([\n",
    "            (pl.col('rel_age') - pl.col('rel_age').shift(shift)).alias('prev_age_delta')\n",
    "        ])\n",
    "    \n",
    "    new_columns = ['prev_' + col for col in columns_to_fill]\n",
    "\n",
    "    temp_df = temp_df.with_columns([\n",
    "        pl.col(col).shift(shift).alias('prev_' + col) for col in columns_to_fill\n",
    "    ])\n",
    "    \n",
    "    if include_age_delta:\n",
    "        new_columns.append('prev_age_delta')\n",
    "    \n",
    "    return df.join(temp_df.select([primary_key] + new_columns), on=primary_key, how='left')\n",
    "\n",
    "pd_df = df.to_pandas()\n",
    "pd_df = pd_fill_prev_txn(pd_df, dt_norm_columns=dt_norm_values, columns_to_fill=['std_txn_type', 'std_txn_method', 'USD_amount'], include_age_delta=True)\n",
    "# df = pl_fill_prev_txn(df, dt_norm_columns=dt_norm_values, columns_to_fill=['std_txn_type', 'USD_amount'], include_age_delta=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class TransactionProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        time_bins: List[int] = [7, 14, 30],\n",
    "        amount_col: str = 'USD_amount',  # Updated\n",
    "        customer_id_col: str = 'party_Id',  # Updated\n",
    "        age_col: str = 'txn_age_days',  # Updated\n",
    "        reporting_threshold: int = 10000 # Added\n",
    "    ):\n",
    "        self.time_bins = time_bins\n",
    "        self.amount_col = amount_col\n",
    "        self.customer_id_col = customer_id_col\n",
    "        self.age_col = age_col\n",
    "        self.reporting_threshold = reporting_threshold\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_customer_metrics(args: Tuple[pd.DataFrame, Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Calculate metrics for a single customer's transactions\"\"\"\n",
    "        customer_data, config = args\n",
    "        result = customer_data.copy()\n",
    "        \n",
    "        # Get configuration\n",
    "        time_bins = config['time_bins']\n",
    "        amount_col = config['amount_col']\n",
    "        age_col = config['age_col']\n",
    "        threshold = config.get('reporting_threshold', 10000)  # Add to config\n",
    "\n",
    "        \n",
    "        # Sort by age\n",
    "        result = result.sort_values(age_col)\n",
    "\n",
    "        # Calculate suspicious range (80-99.9% of threshold)\n",
    "        suspicious_lower = threshold * 0.8\n",
    "        suspicious_upper = threshold\n",
    "\n",
    "        \n",
    "        # Calculate metrics for each time bin\n",
    "        for days in time_bins:\n",
    "            result[f'volume_{days}d_sum'] = 0.0\n",
    "            result[f'velocity_{days}d_count'] = 0\n",
    "            result[f'stat_{days}d_median'] = 0.0\n",
    "            result[f'stat_{days}d_mad'] = 0.0        \n",
    "            result[f'under_threshold_{days}d_count'] = 0  # New metric\n",
    "            result[f'under_threshold_{days}d_sum'] = 0.0  # New metric\n",
    "\n",
    "            for idx in range(len(result)):\n",
    "                current_age = result.iloc[idx][age_col]\n",
    "                mask = (\n",
    "                    (result[age_col] <= (current_age - 1)) & \n",
    "                    (result[age_col] > (current_age - days - 1))\n",
    "                )\n",
    "                window_data = result.loc[mask, amount_col]\n",
    "                suspicious_data = window_data[\n",
    "                    (window_data >= suspicious_lower) & \n",
    "                    (window_data < suspicious_upper)\n",
    "                ]\n",
    "\n",
    "                if len(window_data) > 0:\n",
    "                    result.iloc[idx, result.columns.get_loc(f'volume_{days}d_sum')] = window_data.sum()\n",
    "                    median_ = window_data.median()\n",
    "                    mad_ = np.median([abs(x - median_) for x in window_data])\n",
    "                    result.iloc[idx, result.columns.get_loc(f'stat_{days}d_median')] = median_\n",
    "                    result.iloc[idx, result.columns.get_loc(f'stat_{days}d_mad')] = mad_\n",
    "                    result.iloc[idx, result.columns.get_loc(f'velocity_{days}d_count')] = len(window_data)\n",
    "                    result.iloc[idx, result.columns.get_loc(f'under_threshold_{days}d_count')] = len(suspicious_data)\n",
    "                    result.iloc[idx, result.columns.get_loc(f'under_threshold_{days}d_sum')] = suspicious_data.sum()\n",
    "\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def process_transactions(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        n_jobs: int = None,\n",
    "        batch_size: int = 1000\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Process all transactions using parallel processing\"\"\"\n",
    "        if n_jobs is None:\n",
    "            n_jobs = max(1, mp.cpu_count() - 1)\n",
    "        \n",
    "        print(f\"Starting processing with {n_jobs} workers\")\n",
    "        print(f\"Total transactions: {len(df):,}\")\n",
    "        print(f\"Total customers: {df[self.customer_id_col].nunique():,}\")\n",
    "        \n",
    "        # Sort data\n",
    "        df = df.sort_values([self.customer_id_col, self.age_col])\n",
    "        \n",
    "        # Split customers into batches\n",
    "        all_customers = df[self.customer_id_col].unique()\n",
    "        customer_batches = [\n",
    "            all_customers[i:i + batch_size] \n",
    "            for i in range(0, len(all_customers), batch_size)\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        config = {\n",
    "            'time_bins': self.time_bins,\n",
    "            'amount_col': self.amount_col,\n",
    "            'age_col': self.age_col,\n",
    "            'reporting_threshold': self.reporting_threshold\n",
    "        }\n",
    "        \n",
    "        print(f\"Processing {len(customer_batches)} batches...\")\n",
    "        \n",
    "        for batch_idx, customer_batch in enumerate(customer_batches, 1):\n",
    "            print(f\"\\nProcessing batch {batch_idx}/{len(customer_batches)}\")\n",
    "            \n",
    "            # Get data for current batch\n",
    "            batch_data = df[df[self.customer_id_col].isin(customer_batch)]\n",
    "            \n",
    "            # Process this batch without multiprocessing\n",
    "            batch_results = []\n",
    "            for _, group in tqdm(batch_data.groupby(self.customer_id_col), \n",
    "                               desc=\"Processing customers in batch\"):\n",
    "                result = self.calculate_customer_metrics((group, config))\n",
    "                batch_results.append(result)\n",
    "            \n",
    "            # Combine batch results\n",
    "            if batch_results:\n",
    "                batch_df = pd.concat(batch_results, ignore_index=True)\n",
    "                results.append(batch_df)\n",
    "            \n",
    "            # Save intermediate results\n",
    "            if batch_idx % 5 == 0:  # Save every 5 batches\n",
    "                interim_df = pd.concat(results, ignore_index=True)\n",
    "                interim_df.to_parquet(f'interim_results_batch_{batch_idx}.parquet')\n",
    "            \n",
    "            # Clear memory\n",
    "            del batch_data, batch_results\n",
    "            gc.collect()\n",
    "        \n",
    "        # Combine all results\n",
    "        print(\"\\nCombining all results...\")\n",
    "        result_df = pd.concat(results, ignore_index=True)\n",
    "        \n",
    "        # Calculate ratios\n",
    "        # print(\"Calculating ratios...\")\n",
    "        # for days1, days2 in [(7, 14), (7, 30), (14, 30)]:\n",
    "        #     # Average ratio\n",
    "        #     result_df[f'volume_avg_{days1}d_to_{days2}d_ratio'] = (\n",
    "        #         result_df[f'volume_{days1}d_avg'] / \n",
    "        #         result_df[f'volume_{days2}d_avg'].replace(0, np.nan)\n",
    "        #     ).fillna(0)\n",
    "            \n",
    "        #     # Count ratio\n",
    "        #     result_df[f'volume_count_{days1}d_to_{days2}d_ratio'] = (\n",
    "        #         (result_df[f'volume_{days1}d_count'] / days1) /\n",
    "        #         (result_df[f'volume_{days2}d_count'] / days2).replace(0, np.nan)\n",
    "        #     ).fillna(0)\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "def process_with_monitoring(\n",
    "    df: pd.DataFrame,\n",
    "    time_bins: List[int] = [7, 14, 30],\n",
    "    batch_size: int = 1000\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Process with monitoring\"\"\"\n",
    "    import psutil\n",
    "    import os\n",
    "    \n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    print(f\"Start time: {start_time}\")\n",
    "    print(f\"Initial memory usage: {process.memory_info().rss / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize processor\n",
    "        processor = TransactionProcessor(\n",
    "            time_bins=time_bins,\n",
    "            amount_col='USD_amount',\n",
    "            customer_id_col='party_Id',\n",
    "            age_col='txn_age_days',\n",
    "            reporting_threshold=10000\n",
    "        )\n",
    "        \n",
    "        # Process data\n",
    "        result_df = processor.process_transactions(\n",
    "            df=df,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        print(\"\\nProcessing completed successfully!\")\n",
    "        print(f\"End time: {end_time}\")\n",
    "        print(f\"Total processing time: {processing_time}\")\n",
    "        print(f\"Final memory usage: {process.memory_info().rss / 1024 / 1024:.2f} MB\")\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during processing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Read your data\n",
    "    df = pd_df.copy()  # or however you load your data\n",
    "    \n",
    "    # Process with monitoring\n",
    "    result_df = process_with_monitoring(\n",
    "        df=df,\n",
    "        time_bins=[7, 14, 30],\n",
    "        batch_size=1000  # Process 1000 customers at a time\n",
    "    )\n",
    "    \n",
    "    # Save final results\n",
    "    result_df.to_parquet('../data/jp_morgan/transaction_metrics_final_fraud_v3.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculator = TransactionStatsCalculator(\n",
    "        time_bins=[7, 14, 30],\n",
    "        metrics=['avg', 'std', 'count'],\n",
    "        ratio_pairs=[(7, 14), (7, 30), (14, 30)]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = calculator.process_transactions(\n",
    "    df=aml_data,\n",
    "    amount_col='USD_amount',\n",
    "    customer_id_col='party_Id',\n",
    "    age_col='txn_age_days'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_parquet('../data/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_data['USD_amount'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_b = aml_data[:10].copy()\n",
    "test_b.columns = ['b_' + x for x in test_b.columns]\n",
    "test_a = aml_data[:10].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = aml_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feature_time(df, 'Time_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Time_step']\n",
    "timestamp = pd.to_datetime(df['Time_step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_senders = aml_data[['Sender_Id', 'Sender_Account']].drop_duplicates().\n",
    "fraud_senders = fraud_data[['Sender_Id', 'Sender_Account']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_df(df):\n",
    "    df['date'] = pd.to_datetime(df['Time_step'])\n",
    "    # Extract the year and month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "\n",
    "    # Plot the distribution of months per year\n",
    "    df.groupby(['year', 'month']).size().unstack().plot(kind='bar', stacked=True)\n",
    "\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Count of Entries')\n",
    "    plt.title('Month Distribution per Year')\n",
    "    plt.show()\n",
    "\n",
    "plot_hist_df(aml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_data['Label'].map({'GOOD': 0, 'BAD': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_checkpoints(df):\n",
    "    # Create checkpoint directory\n",
    "    import os\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        result_df = process_with_monitoring(\n",
    "            df=df,\n",
    "            customer_id_col='party_Id',\n",
    "            amount_col='USD_amount',\n",
    "            age_col='txn_age_days',\n",
    "            n_jobs=mp.cpu_count() - 1  # Use all but one CPU\n",
    "        )\n",
    "        \n",
    "        # Save final result\n",
    "        result_df.to_parquet('checkpoints/final_result.parquet')\n",
    "        print(\"Results saved to 'checkpoints/final_result.parquet'\")\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcessing interrupted by user\")\n",
    "        return None\n",
    "    \n",
    "process_with_checkpoints(aml_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
