{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, matthews_corrcoef, confusion_matrix\n",
    "import time\n",
    "\n",
    "class CustomFunctionTransformer(FunctionTransformer):\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return input_features\n",
    "\n",
    "# Define the preprocessing steps as functions\n",
    "def replace_inf(df):\n",
    "    df_encoded = df.replace([np.inf], 1e6).replace([-np.inf], -1e6)\n",
    "    if 'prev_age_delta' in df_encoded.columns:\n",
    "        df_encoded['prev_age_delta'] = df_encoded['prev_age_delta'].fillna(0)\n",
    "    if 'prev_USD_amount' in df_encoded.columns:\n",
    "        df_encoded['prev_USD_amount'] = df_encoded['prev_USD_amount'].fillna(0)\n",
    "    return df_encoded\n",
    "\n",
    "def pipeline_init_(model, numerical_columns, categorical_columns):\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessing', ColumnTransformer([\n",
    "            ('num_pipeline', Pipeline([\n",
    "                ('replace_inf', CustomFunctionTransformer(replace_inf)),\n",
    "                ('scaler', MinMaxScaler())\n",
    "            ]), numerical_columns),\n",
    "            \n",
    "            ('cat_pipeline', Pipeline([\n",
    "                ('ohe', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "            ]), categorical_columns)\n",
    "        ])),\n",
    "        ('smote', SMOTE()),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def get_features(feature_selection_df, n=20):\n",
    "    typological_features = [\n",
    "        'is_crossborder', \n",
    "        'under_threshold_14d_count',\n",
    "        'under_threshold_14d_sum',\n",
    "        'under_threshold_30d_count',\n",
    "        'under_threshold_30d_sum',\n",
    "        'under_threshold_7d_count',\n",
    "        'under_threshold_7d_sum'\n",
    "        ]\n",
    "    def_time_columns = ['txn_time_hr', 'txn_time_mm']\n",
    "    def_categorical_columns = ['std_txn_type', 'std_txn_method', 'prev_std_txn_type', 'prev_std_txn_method']\n",
    "\n",
    "\n",
    "    features = feature_selection_df[~feature_selection_df['Feature'].isin(typological_features)].nlargest(n, 'MI_Score').iloc[:,0].tolist() + typological_features\n",
    "\n",
    "    categorical_cols = list(set(features) & set(def_categorical_columns))\n",
    "    numerical_cols = list(set(features) - (set(categorical_cols) | set(def_time_columns)))\n",
    "\n",
    "    return features, categorical_cols, numerical_cols\n",
    "\n",
    "def calculate_informedness_markedness(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Calculate components\n",
    "    sensitivity = tp / (tp + fn)  # also called TPR\n",
    "    specificity = tn / (tn + fp)  # also called TNR\n",
    "    ppv = tp / (tp + fp)  # positive predictive value\n",
    "    npv = tn / (tn + fn)  # negative predictive value\n",
    "    \n",
    "    # Calculate metrics\n",
    "    informedness = sensitivity + specificity - 1\n",
    "    markedness = ppv + npv - 1\n",
    "    \n",
    "    return informedness, markedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet('../data/split/resplit/ds1_train.parquet').drop(columns=['Time_step', 'Transaction_Id', 'Transaction_Type','party_Id',\n",
    "       'party_Account', 'party_Country', 'cparty_Id', 'cparty_Account',\n",
    "       'cparty_Country',])\n",
    "df_test = pd.read_parquet('../data/split/resplit/ds1_test.parquet').drop(columns=['Time_step', 'Transaction_Id', 'Transaction_Type','party_Id',\n",
    "       'party_Account', 'party_Country', 'cparty_Id', 'cparty_Account',\n",
    "       'cparty_Country',])\n",
    "\n",
    "X_train  = df_train.drop(columns=['Label'])\n",
    "y_train = df_train['Label']\n",
    "\n",
    "X_test  = df_test.drop(columns=['Label'])\n",
    "y_test = df_test['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.9962149097370446\n",
      "MCC: 0.8468797461001761\n",
      "Informedness: 0.8628492798902188\n",
      "Markedness: 0.8312057749482615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.88      0.93    250813\n",
      "           1       0.84      0.99      0.91    166477\n",
      "\n",
      "    accuracy                           0.92    417290\n",
      "   macro avg       0.92      0.93      0.92    417290\n",
      "weighted avg       0.93      0.92      0.92    417290\n",
      "\n",
      "Time to train: 4.831802845001221\n"
     ]
    }
   ],
   "source": [
    "selected_features_, categorical_cols, numerical_cols = get_features(pd.read_csv('../data/feature_selection/ds1_mi_to_target.csv'))\n",
    "pipeline = pipeline_init_(MultinomialNB(), numerical_cols, categorical_cols)\n",
    "actual_features = pipeline['preprocessing'].get_feature_names_out()\n",
    "\n",
    "# Time\n",
    "start_time = time.time()\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Time\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "informedness, markedness = calculate_informedness_markedness(y_test, y_pred)\n",
    "\n",
    "print(f'ROC AUC: {roc_auc}')\n",
    "print(f'MCC: {mcc}')\n",
    "print(f'Informedness: {informedness}')\n",
    "print(f'Markedness: {markedness}')\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Time to train: {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = pipeline['preprocessing'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['party_entity_btw',\n",
       " 'party_account_btw',\n",
       " 'volume_30d_sum',\n",
       " 'USD_amount',\n",
       " 'stat_30d_median',\n",
       " 'prev_USD_amount',\n",
       " 'volume_14d_sum',\n",
       " 'stat_30d_mad',\n",
       " 'stat_14d_median',\n",
       " 'party_entity_deg',\n",
       " 'cparty_is_account',\n",
       " 'volume_7d_sum',\n",
       " 'stat_7d_median',\n",
       " 'cparty_is_cash',\n",
       " 'is_credit',\n",
       " 'stat_14d_mad',\n",
       " 'std_txn_method',\n",
       " 'cparty_l1_btw',\n",
       " 'cparty_l2_btw',\n",
       " 'std_txn_type',\n",
       " 'is_crossborder',\n",
       " 'under_threshold_14d_count',\n",
       " 'under_threshold_14d_sum',\n",
       " 'under_threshold_30d_count',\n",
       " 'under_threshold_30d_sum',\n",
       " 'under_threshold_7d_count',\n",
       " 'under_threshold_7d_sum']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = pipeline.named_steps['classifier']\n",
    "# importance = np.std(classifier_model.feature_log_prob_, axis=0)\n",
    "log_probs = classifier_model.feature_log_prob_\n",
    "importance = log_probs[0] - log_probs[1]\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -4.22803479,  -4.55227087,  -2.89864037,  -8.60963631,\n",
       "         -9.73075464,  -9.04198807,  -2.28913596,  -4.58981874,\n",
       "         -8.40503246,  -3.4167876 ,  -7.85409939,  -8.30765452,\n",
       "         -2.82360589,  -8.88644533,  -2.31699791,  -8.32095053,\n",
       "         -1.85587061,  -8.49497763,  -9.18554656,  -9.42263553,\n",
       "         -5.11513714,  -2.3596165 ,  -3.24718018,  -5.152627  ,\n",
       "         -4.19042248,  -1.90427506,  -1.93410267,  -5.21543678],\n",
       "       [ -9.96318842, -10.15762792,  -3.73431381,  -3.97258017,\n",
       "         -4.82752828,  -4.40974318,  -2.23435767, -10.21424743,\n",
       "         -3.75260063,  -3.00901379,  -3.02302881,  -3.51250406,\n",
       "         -2.54242742,  -4.20798509,  -2.15516467,  -3.53630949,\n",
       "         -1.82765922,  -3.8179026 ,  -4.31163663,  -4.77294361,\n",
       "        -10.67457474,  -2.70777482,  -3.40256105, -10.73338213,\n",
       "         -9.90266886,  -2.70777482,  -2.73438994,  -5.5749223 ]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_model.feature_log_prob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -5.19377172,  -5.49181494,  -8.14665391, -10.27246556,\n",
       "         -8.34967788,  -2.6539073 ,  -2.71013955,  -5.53024236,\n",
       "         -9.31133522,  -2.8301567 ,  -2.97319844,  -9.20441569,\n",
       "         -2.87107645,  -9.71975885,  -2.5030185 ,  -9.21568436,\n",
       "         -2.24976105,  -9.39011991,  -9.90254155,  -8.59324597,\n",
       "         -6.03499244,  -2.21984527,  -2.72527614,  -6.0735403 ,\n",
       "         -5.15548723,  -3.61990029,  -2.11760785,  -2.6476801 ,\n",
       "         -2.86354457,  -3.36439281],\n",
       "       [ -8.79146797,  -8.80561891,  -4.0090128 ,  -4.86376091,\n",
       "         -4.4444464 ,  -2.56835548,  -2.18362747,  -8.85582926,\n",
       "         -3.79171437,  -3.34936465,  -3.3848866 ,  -3.54885944,\n",
       "         -2.62250484,  -4.24509428,  -2.73182338,  -3.5770105 ,\n",
       "         -2.56618343,  -3.85812242,  -4.34766738,  -4.80368822,\n",
       "         -9.12917571,  -2.45054307,  -3.07516267,  -9.18240779,\n",
       "         -8.73924316,  -4.76907758,  -2.48748114,  -3.51182003,\n",
       "         -2.87521005,  -3.15586188]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_model.feature_log_prob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mscds_dsrtn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
