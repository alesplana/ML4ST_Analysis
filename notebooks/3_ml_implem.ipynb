{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure all data have been scaled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic data to test if code runs correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the input data\n",
    "scaler = MinMaxScaler((0,1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All input data must be scaled\n",
    "def lr_train(X_train, y_train):\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "def test_model(model, X_test, y_test):\n",
    "    y_predicted = model.predict(X_test)\n",
    "\n",
    "    accuracy_result = accuracy_score(y_test, y_predicted)\n",
    "    confusion_matrix_result = confusion_matrix(y_test, y_predicted)\n",
    "    classification_report_result = classification_report(y_test, y_predicted)\n",
    "\n",
    "    return accuracy_result, confusion_matrix_result, classification_report_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.835,\n",
       " array([[88, 18],\n",
       "        [15, 79]]),\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.85      0.83      0.84       106\\n           1       0.81      0.84      0.83        94\\n\\n    accuracy                           0.83       200\\n   macro avg       0.83      0.84      0.83       200\\nweighted avg       0.84      0.83      0.84       200\\n')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LR = lr_train(X_train, y_train)\n",
    "test_model(model_LR, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes (MultinomialNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All input data must be scaled\n",
    "def mnb_train(X_train, y_train):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.74,\n",
       " array([[75, 31],\n",
       "        [21, 73]]),\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.78      0.71      0.74       106\\n           1       0.70      0.78      0.74        94\\n\\n    accuracy                           0.74       200\\n   macro avg       0.74      0.74      0.74       200\\nweighted avg       0.74      0.74      0.74       200\\n')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_MNB = mnb_train(X_train, y_train)\n",
    "test_model(model_MNB, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, global_mean_pool\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "class GraphFeatureExtractor:\n",
    "    def __init__(self, G: nx.Graph):\n",
    "        self.G = G\n",
    "        \n",
    "    def extract_node_features(self, node_id: str) -> np.array:\n",
    "        \"\"\"Extract features for a given node\"\"\"\n",
    "        node = self.G.nodes[node_id]\n",
    "        features = []\n",
    "        \n",
    "        # Node type one-hot encoding\n",
    "        node_type = node.get('type', '')\n",
    "        type_encoding = [0, 0, 0, 0]  # Entity, Account, Standalone, Cash\n",
    "        if node_type == 'Entity': type_encoding[0] = 1\n",
    "        elif node_type == 'Account': type_encoding[1] = 1\n",
    "        elif node_type == 'Standalone': type_encoding[2] = 1\n",
    "        elif node_type == 'Cash Node': type_encoding[3] = 1\n",
    "        features.extend(type_encoding)\n",
    "        \n",
    "        # Add other node attributes\n",
    "        if node_type in ['Entity', 'Standalone']:\n",
    "            features.append(1 if node.get('country') else 0)\n",
    "        else:\n",
    "            features.append(0)\n",
    "            \n",
    "        return np.array(features)\n",
    "    \n",
    "    def extract_edge_features(self, edge: Tuple) -> np.array:\n",
    "        \"\"\"Extract features for a given edge\"\"\"\n",
    "        edge_data = self.G.edges[edge]\n",
    "        features = []\n",
    "        \n",
    "        # Edge type one-hot encoding\n",
    "        edge_type = edge_data.get('type', '')\n",
    "        type_encoding = [0, 0]  # Account Ownership, Transaction\n",
    "        if edge_type == 'Account Ownership': type_encoding[0] = 1\n",
    "        elif edge_type == 'Transaction': type_encoding[1] = 1\n",
    "        features.extend(type_encoding)\n",
    "        \n",
    "        # Transaction-specific features\n",
    "        if edge_type == 'Transaction':\n",
    "            features.extend([\n",
    "                edge_data.get('amount', 0),\n",
    "                edge_data.get('txn time', 0) // 100,  # hour\n",
    "                edge_data.get('txn time', 0) % 100,   # minute\n",
    "                edge_data.get('entity_age', 0),\n",
    "            ])\n",
    "        else:\n",
    "            features.extend([0, 0, 0, 0])\n",
    "            \n",
    "        return np.array(features)\n",
    "\n",
    "class GraphRandomForest:\n",
    "    def __init__(self, n_estimators=100):\n",
    "        self.rf = RandomForestClassifier(n_estimators=n_estimators)\n",
    "        self.feature_extractor = None\n",
    "        \n",
    "    def prepare_features(self, G: nx.Graph, target_type: str) -> Tuple[np.array, np.array]:\n",
    "        \"\"\"Prepare features for either transaction or entity classification\"\"\"\n",
    "        self.feature_extractor = GraphFeatureExtractor(G)\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        if target_type == 'transaction':\n",
    "            for edge in G.edges():\n",
    "                if G.edges[edge].get('type') == 'Transaction':\n",
    "                    # Extract features from connected nodes and edge\n",
    "                    node1_feat = self.feature_extractor.extract_node_features(edge[0])\n",
    "                    node2_feat = self.feature_extractor.extract_node_features(edge[1])\n",
    "                    edge_feat = self.feature_extractor.extract_edge_features(edge)\n",
    "                    \n",
    "                    combined_feat = np.concatenate([node1_feat, node2_feat, edge_feat])\n",
    "                    features.append(combined_feat)\n",
    "                    labels.append(G.edges[edge].get('suspicious', 0))\n",
    "                    \n",
    "        elif target_type == 'entity':\n",
    "            for node in G.nodes():\n",
    "                if G.nodes[node].get('type') in ['Entity', 'Standalone']:\n",
    "                    # Extract node features and aggregate connected transaction features\n",
    "                    node_feat = self.feature_extractor.extract_node_features(node)\n",
    "                    \n",
    "                    # Aggregate transaction features\n",
    "                    txn_features = []\n",
    "                    for edge in G.edges(node):\n",
    "                        if G.edges[edge].get('type') == 'Transaction':\n",
    "                            txn_features.append(self.feature_extractor.extract_edge_features(edge))\n",
    "                    \n",
    "                    if txn_features:\n",
    "                        txn_agg = np.mean(txn_features, axis=0)\n",
    "                    else:\n",
    "                        txn_agg = np.zeros(6)  # Default edge feature size\n",
    "                        \n",
    "                    combined_feat = np.concatenate([node_feat, txn_agg])\n",
    "                    features.append(combined_feat)\n",
    "                    labels.append(G.nodes[node].get('suspicious', 0))\n",
    "                    \n",
    "        return np.array(features), np.array(labels)\n",
    "    \n",
    "    def fit(self, G: nx.Graph, target_type: str):\n",
    "        X, y = self.prepare_features(G, target_type)\n",
    "        self.rf.fit(X, y)\n",
    "        \n",
    "    def predict(self, G: nx.Graph, target_type: str) -> np.array:\n",
    "        X, _ = self.prepare_features(G, target_type)\n",
    "        return self.rf.predict(X)\n",
    "\n",
    "class GraphGradientBoosting:\n",
    "    def __init__(self, n_estimators=100):\n",
    "        self.gb = GradientBoostingClassifier(n_estimators=n_estimators)\n",
    "        self.feature_extractor = None\n",
    "        \n",
    "    # Reuse the same feature preparation methods as RandomForest\n",
    "    prepare_features = GraphRandomForest.prepare_features\n",
    "        \n",
    "    def fit(self, G: nx.Graph, target_type: str):\n",
    "        X, y = self.prepare_features(G, target_type)\n",
    "        self.gb.fit(X, y)\n",
    "        \n",
    "    def predict(self, G: nx.Graph, target_type: str) -> np.array:\n",
    "        X, _ = self.prepare_features(G, target_type)\n",
    "        return self.gb.predict(X)\n",
    "\n",
    "class GraphNeuralNetwork(nn.Module):\n",
    "    def __init__(self, node_features, edge_features, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        self.node_classifier = nn.Linear(hidden_channels, 1)\n",
    "        self.edge_classifier = nn.Linear(hidden_channels * 2 + edge_features, 1)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr=None, predict_edges=False):\n",
    "        # Node embedding\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        node_embeddings = F.relu(x)\n",
    "        \n",
    "        if predict_edges:\n",
    "            # For edge prediction, combine node embeddings and edge features\n",
    "            src, dst = edge_index\n",
    "            edge_features = torch.cat([\n",
    "                node_embeddings[src],\n",
    "                node_embeddings[dst],\n",
    "                edge_attr\n",
    "            ], dim=1)\n",
    "            return torch.sigmoid(self.edge_classifier(edge_features))\n",
    "        else:\n",
    "            # For node prediction\n",
    "            return torch.sigmoid(self.node_classifier(node_embeddings))\n",
    "\n",
    "class GraphGNNWrapper:\n",
    "    def __init__(self, node_features, edge_features, hidden_channels=64):\n",
    "        self.model = GraphNeuralNetwork(node_features, edge_features, hidden_channels)\n",
    "        self.feature_extractor = None\n",
    "        \n",
    "    def prepare_pyg_data(self, G: nx.Graph) -> Data:\n",
    "        \"\"\"Convert NetworkX graph to PyTorch Geometric Data\"\"\"\n",
    "        self.feature_extractor = GraphFeatureExtractor(G)\n",
    "        \n",
    "        # Prepare node features and create node mapping\n",
    "        node_mapping = {node: idx for idx, node in enumerate(G.nodes())}\n",
    "        node_features = []\n",
    "        for node in G.nodes():\n",
    "            node_features.append(self.feature_extractor.extract_node_features(node))\n",
    "        \n",
    "        # Prepare edge indices and features\n",
    "        edge_index = []\n",
    "        edge_features = []\n",
    "        for edge in G.edges():\n",
    "            # Add edge in both directions for undirected graphs\n",
    "            edge_index.extend([[node_mapping[edge[0]], node_mapping[edge[1]]],\n",
    "                             [node_mapping[edge[1]], node_mapping[edge[0]]]])\n",
    "            edge_feat = self.feature_extractor.extract_edge_features(edge)\n",
    "            edge_features.extend([edge_feat, edge_feat])\n",
    "        \n",
    "        return Data(\n",
    "            x=torch.FloatTensor(node_features),\n",
    "            edge_index=torch.LongTensor(edge_index).t().contiguous(),\n",
    "            edge_attr=torch.FloatTensor(edge_features)\n",
    "        )\n",
    "    \n",
    "    def fit(self, G: nx.Graph, target_type: str, epochs=100):\n",
    "        data = self.prepare_pyg_data(G)\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if target_type == 'transaction':\n",
    "                # Process edges in the same order as they were added to edge_index\n",
    "                edge_labels = []\n",
    "                for i in range(0, len(data.edge_index.t()), 2):  # Step by 2 since we added edges in both directions\n",
    "                    src = data.edge_index[0][i].item()\n",
    "                    dst = data.edge_index[1][i].item()\n",
    "                    # Convert node indices back to original IDs\n",
    "                    src_id = list(G.nodes())[src]\n",
    "                    dst_id = list(G.nodes())[dst]\n",
    "                    edge_data = G.get_edge_data(src_id, dst_id)\n",
    "                    if edge_data and edge_data.get('type') == 'Transaction':\n",
    "                        edge_labels.extend([edge_data.get('suspicious', 0)] * 2)  # For both directions\n",
    "                    else:\n",
    "                        edge_labels.extend([0] * 2)  # Non-transaction edges\n",
    "                \n",
    "                out = self.model(data.x, data.edge_index, data.edge_attr, predict_edges=True)\n",
    "                labels = torch.FloatTensor(edge_labels)\n",
    "            else:  # entity\n",
    "                out = self.model(data.x, data.edge_index)\n",
    "                # Process nodes in the same order as they appear in the graph\n",
    "                node_labels = []\n",
    "                for node_id in G.nodes():\n",
    "                    if G.nodes[node_id].get('type') in ['Entity', 'Standalone']:\n",
    "                        node_labels.append(G.nodes[node_id].get('suspicious', 0))\n",
    "                    else:\n",
    "                        node_labels.append(0)\n",
    "                labels = torch.FloatTensor(node_labels)\n",
    "            \n",
    "            loss = F.binary_cross_entropy(out.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def predict(self, G: nx.Graph, target_type: str) -> np.array:\n",
    "        data = self.prepare_pyg_data(G)\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if target_type == 'transaction':\n",
    "                out = self.model(data.x, data.edge_index, data.edge_attr, predict_edges=True)\n",
    "            else:  # entity\n",
    "                out = self.model(data.x, data.edge_index)\n",
    "                \n",
    "        return (out.squeeze().numpy() > 0.5).astype(int)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes\n",
    "    G.add_node('E1', type='Entity', country='US')\n",
    "    G.add_node('A1', type='Account', parent_Entity_Id='E1')\n",
    "    G.add_node('S1', type='Standalone', country='UK')\n",
    "    G.add_node('C1', type='Cash Node')\n",
    "    \n",
    "    # Add edges with both suspicious and non-suspicious transactions\n",
    "    G.add_edge('E1', 'A1', type='Account Ownership')\n",
    "    G.add_edge('E1', 'S1', type='Transaction', amount=1000, txn_time=1430, \n",
    "               entity_age=365, transaction_type=1, suspicious=1)\n",
    "    G.add_edge('A1', 'C1', type='Transaction', amount=500, txn_time=1530,\n",
    "               entity_age=365, transaction_type=0, suspicious=0)\n",
    "    \n",
    "    # Initialize models\n",
    "    rf_model = GraphRandomForest()\n",
    "    gb_model = GraphGradientBoosting()\n",
    "    gnn_model = GraphGNNWrapper(node_features=5, edge_features=6)\n",
    "    \n",
    "    # Train models\n",
    "    target_type = 'transaction'  # or 'entity'\n",
    "    rf_model.fit(G, target_type)\n",
    "    gb_model.fit(G, target_type)\n",
    "    gnn_model.fit(G, target_type)\n",
    "    \n",
    "    # Make predictions\n",
    "    rf_pred = rf_model.predict(G, target_type)\n",
    "    gb_pred = gb_model.predict(G, target_type)\n",
    "    gnn_pred = gnn_model.predict(G, target_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnn_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
