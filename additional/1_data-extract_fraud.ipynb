{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "# Paths\n",
    "data_dir = '../data/jp_morgan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# aml_data = pd.read_csv(f'{data_dir}/aml_syn_data.csv')\n",
    "fraud_data = pd.read_csv(f'{data_dir}/fraud_payment_data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hybrid_Txn_Type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/AMIEL/[Personal] Git/.conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Hybrid_Txn_Type'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfraud_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHybrid_Txn_Type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdrop_duplicates()\n",
      "File \u001b[0;32m~/Documents/AMIEL/[Personal] Git/.conda/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Documents/AMIEL/[Personal] Git/.conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Hybrid_Txn_Type'"
     ]
    }
   ],
   "source": [
    "fraud_data['Hybrid_Txn_Type'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_data['Old_Txn_Type'] = fraud_data['Transaction_Type']\n",
    "fraud_data['Transaction_Type'] = fraud_data['Transaction_Id'].str.split('-').str[:-1].str.join('-')\n",
    "fraud_data['Transaction_Type'] = fraud_data['Old_Txn_Type'] + '-' + fraud_data['Transaction_Type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_data['Hybrid_Txn_Type'] = fraud_data['Old_Txn_Type'] + '-' + fraud_data['Transaction_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = [\n",
    "    'Time_step',\n",
    "    'Label',\n",
    "    'Transaction_Id',\n",
    "    'Transaction_Type',\n",
    "    'Sender_Id',\n",
    "    'Sender_Account',\n",
    "    'Sender_Country',\n",
    "    'Bene_Id', \n",
    "    'Bene_Account',\n",
    "    'Bene_Country',\n",
    "    'USD_amount',\n",
    "]\n",
    "# aml_data = aml_data[common_columns]\n",
    "fraud_data = fraud_data[common_columns]\n",
    "aml_data = fraud_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "df = pl.DataFrame(aml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_mapping = {\n",
    "  \"QUICK-PAYMENT-QUICK-PAYMENT\": {\n",
    "    \"std_txn_type\": \"PAYMENT\",\n",
    "    \"std_txn_method\": \"ELECTRONIC\"\n",
    "  },\n",
    "  \"DEPOSIT-CASH-DEPOSIT-CASH\": {\n",
    "    \"std_txn_type\": \"DEPOSIT\",\n",
    "    \"std_txn_method\": \"CASH\"\n",
    "  },\n",
    "  \"PAY-CHECK-PAY-CHECK\": {\n",
    "    \"std_txn_type\": \"PAYMENT\",\n",
    "    \"std_txn_method\": \"CHECK\"\n",
    "  },\n",
    "  \"DEPOSIT-CHECK-DEPOSIT-CHECK\": {\n",
    "    \"std_txn_type\": \"DEPOSIT\",\n",
    "    \"std_txn_method\": \"CHECK\"\n",
    "  },\n",
    "  \"MAKE-PAYMENT-MAKE-PAYMENT\": {\n",
    "    \"std_txn_type\": \"PAYMENT\",\n",
    "    \"std_txn_method\": \"ELECTRONIC\"\n",
    "  },\n",
    "  \"MOVE-FUNDS-MOVE-FUNDS\": {\n",
    "    \"std_txn_type\": \"TRANSFER\",\n",
    "    \"std_txn_method\": \"ELECTRONIC\"\n",
    "  },\n",
    "  \"DEPOSIT-CASH-QUICK-DEPOSIT\": {\n",
    "    \"std_txn_type\": \"DEPOSIT\",\n",
    "    \"std_txn_method\": \"CASH\"\n",
    "  },\n",
    "  \"DEPOSIT-CHECK-CASH-CHECK\": {\n",
    "    \"std_txn_type\": \"DEPOSIT\",\n",
    "    \"std_txn_method\": \"CHECK\"\n",
    "  },\n",
    "  \"MAKE-PAYMENT-PAY-BILL\": {\n",
    "    \"std_txn_type\": \"PAYMENT\",\n",
    "    \"std_txn_method\": \"ELECTRONIC\"\n",
    "  },\n",
    "  \"WITHDRAWAL-WITHDRAWAL\": {\n",
    "    \"std_txn_type\": \"WITHDRAWAL\",\n",
    "    \"std_txn_method\": \"CASH\"\n",
    "  },\n",
    "  \"MAKE-PAYMENT-PAYMENT\": {\n",
    "    \"std_txn_type\": \"PAYMENT\",\n",
    "    \"std_txn_method\": \"ELECTRONIC\"\n",
    "  },\n",
    "  \"WITHDRAWAL-CASH-CHECK\": {\n",
    "    \"std_txn_type\": \"WITHDRAWAL\",\n",
    "    \"std_txn_method\": \"CHECK\"\n",
    "  },\n",
    "  \"EXCHANGE-CASH-CHECK\": {\n",
    "    \"std_txn_type\": \"WITHDRAWAL\",\n",
    "    \"std_txn_method\": \"CHECK\"\n",
    "  },\n",
    "  \"EXCHANGE-WITHDRAWAL\": {\n",
    "    \"std_txn_type\": \"WITHDRAWAL\",\n",
    "    \"std_txn_method\": \"CHECK\"\n",
    "  },\n",
    "  \"EXCHANGE-EXCHANGE\": {\n",
    "    \"std_txn_type\": \"WITHDRAWAL\",\n",
    "    \"std_txn_method\": \"CHECK\"\n",
    "  },\n",
    "  \"WITHDRAWAL-EXCHANGE\": {\n",
    "    \"std_txn_type\": \"WITHDRAWAL\",\n",
    "    \"std_txn_method\": \"CHECK\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_map_transactions(df: pl.DataFrame, txn_mapping: dict) -> pl.DataFrame:\n",
    "    # Pre-compute the mappings\n",
    "    type_mapping = {k: v['std_txn_type'] for k, v in txn_mapping.items()}\n",
    "    method_mapping = {k: v['std_txn_method'] for k, v in txn_mapping.items()}\n",
    "    \n",
    "    # First check for unmapped values\n",
    "    unique_txn_types = df.get_column('Transaction_Type').unique().to_list()  # Note the column name change\n",
    "    unmapped_txns = [txn for txn in unique_txn_types if txn not in type_mapping]\n",
    "    \n",
    "    if unmapped_txns:\n",
    "        raise ValueError(\n",
    "            f\"Found {len(unmapped_txns)} unmapped transaction types:\\n\"\n",
    "            f\"{unmapped_txns}\\n\"\n",
    "            f\"Please update the mapping dictionary with these values.\"\n",
    "        )\n",
    "    \n",
    "    # If we get here, all values are mapped, so proceed with mapping\n",
    "    return df.with_columns([\n",
    "        pl.col('Transaction_Type').replace(type_mapping).alias('std_txn_type'),\n",
    "        pl.col('Transaction_Type').replace(method_mapping).alias('std_txn_method')\n",
    "    ])\n",
    "\n",
    "df = std_map_transactions(df, txn_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Columns:\n",
    "* hour of transaction\n",
    "* normalised age on transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "1. Filter only transactions considering money movements\n",
    "2. Retable DataFrame to be in Party v Counterparty format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AML Data Preprocessing\n",
    "# Filter only money movements\n",
    "# aml_data = aml_data[aml_data['Transaction_Type'].isin(['WIRE', 'PAYMENT', 'WITHDRAWL', 'CASH-DEPOSIT', 'CRYPTO-TRANSFER'])].reset_index()\n",
    "\n",
    "# Identify if row is reversed for party v counterparty retabling\n",
    "def pd_party_reverse_flag(\n",
    "        df: pd.DataFrame, \n",
    "        sender_col: str = 'Sender_Id', \n",
    "        bene_col: str = 'Bene_Id', \n",
    "        transaction_col: str = 'Transaction_Type'\n",
    "        ) -> pd.DataFrame:\n",
    "\n",
    "    df.loc[:, 'sender_type'] = df[sender_col].str.split('-').str[:2].str.join('-')\n",
    "    df.loc[:, 'benef_type'] = df[bene_col].str.split('-').str[:2].str.join('-')\n",
    "    condition = (\n",
    "        (df[transaction_col] == 'CASH-DEPOSIT') |\n",
    "        (((df[transaction_col] == 'MAKE-PAYMENT') | (df[transaction_col] == 'QUICK-PAYMENT') | (df[transaction_col] == 'MOVE-FUNDS')) & (df['benef_type'] == 'JPMC-CLIENT'))\n",
    "    )\n",
    "    df.loc[:, 'is_credit'] = condition\n",
    "    df.loc[:, 'reversed'] = condition\n",
    "    return df\n",
    "\n",
    "def pl_party_reverse_flag(\n",
    "        df: pl.DataFrame, \n",
    "        sender_col: str = 'Sender_Id', \n",
    "        bene_col: str = 'Bene_Id', \n",
    "        transaction_col: str = 'Transaction_Type'\n",
    "        ) -> pl.DataFrame:\n",
    "\n",
    "    df = df.with_columns([\n",
    "        pl.col(sender_col).str.split('-').list.slice(0, 2).list.join('-').alias('sender_type'),\n",
    "        pl.col(bene_col).str.split('-').list.slice(0, 2).list.join('-').alias('benef_type')\n",
    "    ])\n",
    "    \n",
    "    condition = (\n",
    "        (pl.col(transaction_col) == 'DEPOSIT') |\n",
    "        (((pl.col(transaction_col) == 'PAYMENT') | \n",
    "          (pl.col(transaction_col) == 'TRANSFER')) & \n",
    "         (pl.col('benef_type') == 'JPMC-CLIENT'))\n",
    "    )\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        condition.alias('is_credit'),\n",
    "        condition.alias('reversed')\n",
    "    ])\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = pl_party_reverse_flag(df, transaction_col='std_txn_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create party v counterparty retabling\n",
    "def pd_new_table(\n",
    "        df: pd.DataFrame, \n",
    "        rev_col: str = 'reversed', \n",
    "        retained_cols: Optional[List[str]] = None\n",
    "        ) -> pd.DataFrame:\n",
    "\n",
    "    for type in ('Id', 'Account', 'Country'):\n",
    "        df.loc[:, f'party_{type}'] = df[f'Sender_{type}'].where(~df[rev_col], df[f'Bene_{type}'])\n",
    "        df.loc[:, f'cparty_{type}'] = df[f'Bene_{type}'].where(~df[rev_col], df[f'Sender_{type}'])\n",
    "    retained_cols.extend(['party_Id', 'party_Account', 'party_Country',\n",
    "                          'cparty_Id', 'cparty_Account', 'cparty_Country'])\n",
    "    new_table = df[retained_cols]\n",
    "    return new_table\n",
    "\n",
    "def pl_new_table(\n",
    "        df: pl.DataFrame, \n",
    "        rev_col: str = 'reversed', \n",
    "        retained_cols: Optional[List[str]] = None\n",
    "        ) -> pl.DataFrame:\n",
    "\n",
    "    if retained_cols is None:\n",
    "        retained_cols = []\n",
    "\n",
    "    for type in ('Id', 'Account', 'Country'):\n",
    "        df = df.with_columns([\n",
    "            pl.when(~pl.col(rev_col)).then(pl.col(f'Sender_{type}')).otherwise(pl.col(f'Bene_{type}')).alias(f'party_{type}'),\n",
    "            pl.when(~pl.col(rev_col)).then(pl.col(f'Bene_{type}')).otherwise(pl.col(f'Sender_{type}')).alias(f'cparty_{type}')\n",
    "        ])\n",
    "    \n",
    "    retained_cols.extend(['party_Id', 'party_Account', 'party_Country',\n",
    "                          'cparty_Id', 'cparty_Account', 'cparty_Country'])\n",
    "    \n",
    "    new_table = df.select(retained_cols)\n",
    "    return new_table\n",
    "\n",
    "\n",
    "df = pl_new_table(df, retained_cols=[\n",
    "    'Time_step', \n",
    "    'Label',\n",
    "    'Transaction_Id',\n",
    "    'Transaction_Type',\n",
    "    'std_txn_type',\n",
    "    'std_txn_method',\n",
    "    'is_credit',\n",
    "    'USD_amount'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Normalise dates (change into age of account on transaction) and extract hour/minute for off hours transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise date and extract hour minutes\n",
    "def pd_feature_time(\n",
    "        df: pd.DataFrame, \n",
    "        dt_column: str = 'Time_step'\n",
    "        ) -> pd.DataFrame:\n",
    "    \n",
    "    time_series = pd.to_datetime(df[dt_column])\n",
    "    df['txn_time_hr'] = time_series.dt.hour\n",
    "    df['txn_time_mm'] = time_series.dt.minute\n",
    "    \n",
    "    return df\n",
    "\n",
    "def pd_normalise_date(df, dt_column='Time_step', primary_col='party_Id'):\n",
    "    time_series = pd.to_datetime(df[dt_column]).dt.date\n",
    "    df['txn_age_days'] = (time_series - time_series.groupby(df[primary_col]).transform('min')).astype('timedelta64[ns]').dt.days\n",
    "    \n",
    "    return df\n",
    "\n",
    "def pl_feature_time(\n",
    "        df: pl.DataFrame, \n",
    "        dt_column: str = 'Time_step'\n",
    "        ) -> pl.DataFrame:\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        pl.col(dt_column).str.strptime(pl.Datetime).alias('parsed_time')\n",
    "    ])\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        pl.col('parsed_time').dt.hour().alias('txn_time_hr'),\n",
    "        pl.col('parsed_time').dt.minute().alias('txn_time_mm')\n",
    "    ])\n",
    "    \n",
    "    return df.drop('parsed_time')\n",
    "\n",
    "def pl_normalise_date(\n",
    "        df: pl.DataFrame, \n",
    "        dt_column: str = 'Time_step', \n",
    "        primary_col: str = 'party_Id',\n",
    "        dt_fmt: str = '%Y-%m-%d %H:%M:%S'\n",
    "        ) -> pl.DataFrame:\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        pl.col(dt_column).str.strptime(pl.Date, format=dt_fmt).alias('parsed_date')\n",
    "    ])\n",
    "    \n",
    "    min_dates = df.group_by(primary_col).agg(pl.col('parsed_date').min().alias('min_date'))\n",
    "    df = df.join(min_dates, on=primary_col)\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        (pl.col('parsed_date') - pl.col('min_date')).dt.total_days().alias('txn_age_days')\n",
    "    ])\n",
    "    \n",
    "    return df.drop(['parsed_date', 'min_date'])\n",
    "\n",
    "df = pl_feature_time(df)\n",
    "df = pl_normalise_date(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Collect previous transaction details: (transaction type, amount, if applicable delta between transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract previous transaction details\n",
    "dt_norm_values = {\n",
    "    'day': 'txn_age_days',\n",
    "    'hour': 'txn_time_hr',\n",
    "    'min': 'txn_time_mm',\n",
    "}\n",
    "def pd_fill_prev_txn(\n",
    "        df: pd.DataFrame, \n",
    "        primary_key: str = 'party_Id', \n",
    "        dt_column: str | None = None,\n",
    "        dt_norm_columns: Dict[str, str] | None = None, \n",
    "        columns_to_fill: List[str] = [], \n",
    "        shift: int = 1, \n",
    "        include_age_delta: bool = False\n",
    "        ) -> pd.DataFrame:\n",
    "    \n",
    "    if dt_column != None:\n",
    "        core_columns = [primary_key, dt_column]\n",
    "        temp_df = df[core_columns + columns_to_fill].copy()\n",
    "\n",
    "        temp_df[dt_column] = pd.to_datetime(temp_df[dt_column])\n",
    "        temp_df = temp_df.sort_values([primary_key, dt_column])\n",
    "        \n",
    "        temp_df['prev_age_delta'] = (\n",
    "            (temp_df[dt_column] - temp_df.groupby(primary_key)[dt_column].shift(shift))\n",
    "            .dt.total_seconds())/(24 * 3600)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        dt_norm_cols = [dt_norm_columns['day'], dt_norm_columns['hour'], dt_norm_columns['min']]\n",
    "        core_columns = [primary_key] + dt_norm_cols\n",
    "        temp_df = df[core_columns + columns_to_fill].copy()\n",
    "        \n",
    "        temp_df = temp_df.sort_values(by=[primary_key] + dt_norm_cols, ascending=True)\n",
    "        temp_df['rel_age'] = (\n",
    "            temp_df[dt_norm_columns['day']] + \n",
    "            (temp_df[dt_norm_columns['hour']] / 24) + \n",
    "            (temp_df[dt_norm_columns['min']] / (24 * 60))\n",
    "            )\n",
    "        temp_df['prev_age_delta'] = temp_df['rel_age'] - temp_df.groupby(primary_key)['rel_age'].shift(shift)\n",
    "    \n",
    "    new_columns = ['prev_' + col for col in columns_to_fill]\n",
    "\n",
    "    temp_df[new_columns] = temp_df.groupby(primary_key)[columns_to_fill].shift(shift)\n",
    "    if include_age_delta:\n",
    "        new_columns.append('prev_age_delta')\n",
    "    \n",
    "    return df.merge(temp_df[new_columns], left_index=True, right_index=True)\n",
    "\n",
    "def pl_fill_prev_txn(\n",
    "        df: pl.DataFrame, \n",
    "        primary_key: str = 'party_Id', \n",
    "        dt_column: str | None = None,\n",
    "        dt_norm_columns: Dict[str, str] | None = None, \n",
    "        columns_to_fill: List[str] = [], \n",
    "        shift: int = 1, \n",
    "        include_age_delta: bool = False\n",
    "        ) -> pl.DataFrame:\n",
    "    \n",
    "    if dt_column is not None:\n",
    "        core_columns = [primary_key, dt_column]\n",
    "        temp_df = df.select(core_columns  + columns_to_fill)\n",
    "\n",
    "        temp_df = temp_df.with_columns([\n",
    "            pl.col(dt_column).str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\").alias(dt_column)\n",
    "        ])\n",
    "        temp_df = temp_df.sort([primary_key, dt_column])\n",
    "        \n",
    "        temp_df = temp_df.with_columns([\n",
    "            ((pl.col(dt_column) - pl.col(dt_column).shift(shift)).dt.seconds() / (24 * 3600)).alias('prev_age_delta')\n",
    "        ])\n",
    "        \n",
    "    else:\n",
    "        dt_norm_cols = [dt_norm_columns['day'], dt_norm_columns['hour'], dt_norm_columns['min']]\n",
    "        core_columns = [primary_key] + dt_norm_cols\n",
    "        temp_df = df.select(core_columns + columns_to_fill)\n",
    "        \n",
    "        temp_df = temp_df.sort([primary_key] + dt_norm_cols)\n",
    "        temp_df = temp_df.with_columns([\n",
    "            (pl.col(dt_norm_columns['day']) + \n",
    "             (pl.col(dt_norm_columns['hour']) / 24) + \n",
    "             (pl.col(dt_norm_columns['min']) / (24 * 60))).alias('rel_age')\n",
    "        ])\n",
    "        temp_df = temp_df.with_columns([\n",
    "            (pl.col('rel_age') - pl.col('rel_age').shift(shift)).alias('prev_age_delta')\n",
    "        ])\n",
    "    \n",
    "    new_columns = ['prev_' + col for col in columns_to_fill]\n",
    "\n",
    "    temp_df = temp_df.with_columns([\n",
    "        pl.col(col).shift(shift).alias('prev_' + col) for col in columns_to_fill\n",
    "    ])\n",
    "    \n",
    "    if include_age_delta:\n",
    "        new_columns.append('prev_age_delta')\n",
    "    \n",
    "    return df.join(temp_df.select([primary_key] + new_columns), on=primary_key, how='left')\n",
    "\n",
    "aml_data = df.to_pandas()\n",
    "aml_data = pd_fill_prev_txn(aml_data, dt_norm_columns=dt_norm_values, columns_to_fill=['std_txn_type', 'std_txn_method', 'USD_amount'], include_age_delta=True)\n",
    "# df = pl_fill_prev_txn(df, dt_norm_columns=dt_norm_values, columns_to_fill=['std_txn_type', 'USD_amount'], include_age_delta=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Union, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class TransactionStatsCalculator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        time_bins: List[int] = [7, 14, 30],\n",
    "        metrics: List[str] = ['avg', 'std', 'count'],\n",
    "        ratio_pairs: List[Tuple[int, int]] = [(7, 14), (7, 30), (14, 30)]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the transaction statistics calculator\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        time_bins : List[int]\n",
    "            List of time windows in days to calculate metrics for\n",
    "        metrics : List[str]\n",
    "            List of metrics to calculate\n",
    "        ratio_pairs : List[tuple]\n",
    "            List of time bin pairs to calculate ratios for\n",
    "        \"\"\"\n",
    "        self.time_bins = sorted(time_bins)\n",
    "        self.metrics = metrics\n",
    "        self.ratio_pairs = ratio_pairs\n",
    "        self._validate_ratio_pairs()\n",
    "        \n",
    "    def _validate_ratio_pairs(self) -> None:\n",
    "        \"\"\"Validate that ratio pairs use existing time bins\"\"\"\n",
    "        all_periods = set(self.time_bins)\n",
    "        for from_days, to_days in self.ratio_pairs:\n",
    "            if from_days not in all_periods or to_days not in all_periods:\n",
    "                raise ValueError(\n",
    "                    f\"Ratio pair ({from_days}, {to_days}) uses undefined time bin\"\n",
    "                )\n",
    "\n",
    "    def _validate_inputs(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        amount_col: str,\n",
    "        customer_id_col: str,\n",
    "        age_col: str\n",
    "    ) -> None:\n",
    "        \"\"\"Validate input DataFrame and column names\"\"\"\n",
    "        required_cols = [amount_col, customer_id_col, age_col]\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "    def calculate_window_stats(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        reference_age: float,\n",
    "        window_days: int,\n",
    "        amount_col: str = 'amount',\n",
    "        age_col: str = 'account_age'\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate statistics for a specific time window using account age\n",
    "        \"\"\"\n",
    "        # Calculate window boundaries (up to day-1)\n",
    "        end_age = reference_age - 1  # day-1\n",
    "        start_age = end_age - window_days\n",
    "        \n",
    "        # Filter data for the time window\n",
    "        mask = (df[age_col] <= end_age) & (df[age_col] > start_age)\n",
    "        window_data = df.loc[mask, amount_col]\n",
    "        \n",
    "        stats = {}\n",
    "        if len(window_data) > 0:\n",
    "            if 'avg' in self.metrics:\n",
    "                stats[f'volume_{window_days}d_avg'] = float(window_data.mean())\n",
    "            if 'std' in self.metrics:\n",
    "                stats[f'volume_{window_days}d_std'] = float(window_data.std()) if len(window_data) > 1 else 0.0\n",
    "            if 'count' in self.metrics:\n",
    "                stats[f'volume_{window_days}d_count'] = int(len(window_data))\n",
    "        else:\n",
    "            # Handle empty windows\n",
    "            for metric in self.metrics:\n",
    "                stats[f'volume_{window_days}d_{metric}'] = 0.0\n",
    "                \n",
    "        return stats\n",
    "\n",
    "    def calculate_ratios(\n",
    "        self,\n",
    "        stats: Dict[str, float]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Calculate ratios between different time periods\"\"\"\n",
    "        ratio_stats = {}\n",
    "        \n",
    "        for from_days, to_days in self.ratio_pairs:\n",
    "            # Volume average ratio\n",
    "            if 'avg' in self.metrics:\n",
    "                ratio_name = f'volume_avg_{from_days}d_to_{to_days}d_ratio'\n",
    "                from_avg = stats.get(f'volume_{from_days}d_avg', 0)\n",
    "                to_avg = stats.get(f'volume_{to_days}d_avg', 0)\n",
    "                ratio_stats[ratio_name] = from_avg / to_avg if to_avg != 0 else 0\n",
    "                \n",
    "            # Count ratio (normalized by time period)\n",
    "            if 'count' in self.metrics:\n",
    "                ratio_name = f'volume_count_{from_days}d_to_{to_days}d_ratio'\n",
    "                from_count = stats.get(f'volume_{from_days}d_count', 0)\n",
    "                to_count = stats.get(f'volume_{to_days}d_count', 0)\n",
    "                normalized_ratio = (from_count / from_days) / (to_count / to_days) if to_count != 0 else 0\n",
    "                ratio_stats[ratio_name] = normalized_ratio\n",
    "                \n",
    "        return ratio_stats\n",
    "\n",
    "    def calculate_transaction_stats(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        current_transaction: pd.Series,\n",
    "        amount_col: str = 'amount',\n",
    "        customer_id_col: str = 'customer_id',\n",
    "        age_col: str = 'account_age'\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Calculate all statistics for a single transaction\"\"\"\n",
    "        # Get customer's transactions up to this point\n",
    "        customer_df = df[\n",
    "            (df[customer_id_col] == current_transaction[customer_id_col]) &\n",
    "            (df[age_col] < current_transaction[age_col])\n",
    "        ].copy()\n",
    "        \n",
    "        # Calculate stats for each time bin\n",
    "        all_stats = {}\n",
    "        for days in self.time_bins:\n",
    "            window_stats = self.calculate_window_stats(\n",
    "                df=customer_df,\n",
    "                reference_age=current_transaction[age_col],\n",
    "                window_days=days,\n",
    "                amount_col=amount_col,\n",
    "                age_col=age_col\n",
    "            )\n",
    "            all_stats.update(window_stats)\n",
    "        \n",
    "        # Calculate ratios\n",
    "        ratio_stats = self.calculate_ratios(all_stats)\n",
    "        all_stats.update(ratio_stats)\n",
    "        \n",
    "        return all_stats\n",
    "\n",
    "    def _initialize_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Initialize all metric columns in the DataFrame\"\"\"\n",
    "        # Generate column names\n",
    "        metric_columns = [\n",
    "            f'volume_{days}d_{metric}'\n",
    "            for days in self.time_bins\n",
    "            for metric in self.metrics\n",
    "        ]\n",
    "        ratio_columns = [\n",
    "            f'volume_{metric}_{from_d}d_to_{to_d}d_ratio'\n",
    "            for from_d, to_d in self.ratio_pairs\n",
    "            for metric in ['avg', 'count'] if metric in self.metrics\n",
    "        ]\n",
    "        \n",
    "        # Initialize columns with zeros\n",
    "        for col in metric_columns + ratio_columns:\n",
    "            df[col] = 0.0\n",
    "            \n",
    "        return df\n",
    "\n",
    "    def process_transactions(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        amount_col: str = 'amount',\n",
    "        customer_id_col: str = 'customer_id',\n",
    "        age_col: str = 'account_age'\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Process all transactions and add statistics columns\"\"\"\n",
    "        # Validate inputs\n",
    "        self._validate_inputs(df, amount_col, customer_id_col, age_col)\n",
    "        \n",
    "        # Sort and copy data\n",
    "        df = df.sort_values([customer_id_col, age_col]).copy()\n",
    "        \n",
    "        # Initialize metric columns\n",
    "        df = self._initialize_columns(df)\n",
    "        \n",
    "        # Calculate metrics for each transaction\n",
    "        for idx, row in df.iterrows():\n",
    "            stats = self.calculate_transaction_stats(\n",
    "                df=df,\n",
    "                current_transaction=row,\n",
    "                amount_col=amount_col,\n",
    "                customer_id_col=customer_id_col,\n",
    "                age_col=age_col\n",
    "            )\n",
    "            \n",
    "            # Update row with calculated stats\n",
    "            for metric, value in stats.items():\n",
    "                df.at[idx, metric] = value\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def get_column_names(self) -> List[str]:\n",
    "        \"\"\"Get list of all metric column names\"\"\"\n",
    "        metric_columns = [\n",
    "            f'volume_{days}d_{metric}'\n",
    "            for days in self.time_bins\n",
    "            for metric in self.metrics\n",
    "        ]\n",
    "        ratio_columns = [\n",
    "            f'volume_{metric}_{from_d}d_to_{to_d}d_ratio'\n",
    "            for from_d, to_d in self.ratio_pairs\n",
    "            for metric in ['avg', 'count'] if metric in self.metrics\n",
    "        ]\n",
    "        return metric_columns + ratio_columns\n",
    "\n",
    "# Example usage\n",
    "def example_usage():\n",
    "    # Create sample data\n",
    "    data = {\n",
    "        'customer_id': ['A1', 'A1', 'A1', 'A2', 'A2'],\n",
    "        'amount': [100, 200, 150, 300, 250],\n",
    "        'account_age': [10, 20, 30, 15, 25]  # normalized days\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Initialize calculator with custom configuration\n",
    "    calculator = TransactionStatsCalculator(\n",
    "        time_bins=[7, 14, 30],\n",
    "        metrics=['avg', 'std', 'count'],\n",
    "        ratio_pairs=[(7, 14), (7, 30), (14, 30)]\n",
    "    )\n",
    "    \n",
    "    # Process transactions\n",
    "    result_df = calculator.process_transactions(\n",
    "        df=df,\n",
    "        amount_col='amount',\n",
    "        customer_id_col='customer_id',\n",
    "        age_col='account_age'\n",
    "    )\n",
    "    \n",
    "    # Get column names\n",
    "    columns = calculator.get_column_names()\n",
    "    \n",
    "    return result_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculator = TransactionStatsCalculator(\n",
    "        time_bins=[7, 14, 30],\n",
    "        metrics=['avg', 'std', 'count'],\n",
    "        ratio_pairs=[(7, 14), (7, 30), (14, 30)]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aml_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m test_data \u001b[38;5;241m=\u001b[39m calculator\u001b[38;5;241m.\u001b[39mprocess_transactions(\n\u001b[1;32m----> 2\u001b[0m     df\u001b[38;5;241m=\u001b[39m\u001b[43maml_data\u001b[49m,\n\u001b[0;32m      3\u001b[0m     amount_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSD_amount\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     customer_id_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparty_Id\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     age_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxn_age_days\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'aml_data' is not defined"
     ]
    }
   ],
   "source": [
    "test_data = calculator.process_transactions(\n",
    "    df=aml_data,\n",
    "    amount_col='USD_amount',\n",
    "    customer_id_col='party_Id',\n",
    "    age_col='txn_age_days'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_parquet('../data/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(19300001.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml_data['USD_amount'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch\n",
    "\n",
    "# Loaded variable 'aml_data' from kernel state\n",
    "\n",
    "aml_data['Time_step'] = pd.to_datetime(aml_data['Time_step'])\n",
    "\n",
    "aml_data['txn_dt_min'] = aml_data.groupby('party_Id')['Time_step'].transform('min')\n",
    "aml_data.loc[:, 'txn_age'] = (aml_data['Time_step'].dt.date - aml_data['txn_dt_min'].dt.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = aml_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_prev_txn(\n",
    "        df: pd.DataFrame, \n",
    "        primary_key: str = 'party_Id', \n",
    "        dt_column: str | None = None,\n",
    "        dt_norm_columns: Dict[str, str] | None = None, \n",
    "        columns_to_fill: List[str] = [], \n",
    "        shift: int = 1, \n",
    "        include_age_delta: bool = False\n",
    "        ) -> pd.DataFrame:\n",
    "    \n",
    "    if dt_column != None:\n",
    "        core_columns = [primary_key, dt_column]\n",
    "        temp_df = df[core_columns + columns_to_fill].copy()\n",
    "\n",
    "        temp_df[dt_column] = pd.to_datetime(temp_df[dt_column])\n",
    "        temp_df = temp_df.sort_values([primary_key, dt_column])\n",
    "        \n",
    "        temp_df['prev_age_delta'] = (\n",
    "            (temp_df[dt_column] - temp_df.groupby(primary_key)[dt_column].shift(shift))\n",
    "            .dt.total_seconds())/(24 * 3600)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        dt_norm_cols = [dt_norm_columns['day'], dt_norm_columns['hour'], dt_norm_columns['min']]\n",
    "        core_columns = [primary_key] + dt_norm_cols\n",
    "        temp_df = df[core_columns + columns_to_fill].copy()\n",
    "        \n",
    "        temp_df = temp_df.sort_values(by=[primary_key] + dt_norm_cols, ascending=True)\n",
    "        temp_df['rel_age'] = (\n",
    "            temp_df[dt_norm_columns['day']] + \n",
    "            (temp_df[dt_norm_columns['hour']] / 24) + \n",
    "            (temp_df[dt_norm_columns['min']] / (24 * 60))\n",
    "            )\n",
    "        temp_df['prev_age_delta'] = temp_df['rel_age'] - temp_df.groupby(primary_key)['rel_age'].shift(shift)\n",
    "    \n",
    "    new_columns = ['prev_' + col for col in columns_to_fill]\n",
    "\n",
    "    temp_df[new_columns] = temp_df.groupby(primary_key)[columns_to_fill].shift(shift)\n",
    "    if include_age_delta:\n",
    "        new_columns.append('prev_age_delta')\n",
    "    \n",
    "    return df.merge(temp_df[new_columns], left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1547740258.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    aml_senders = aml_data[['Sender_Id', 'Sender_Account']].drop_duplicates().\u001b[0m\n\u001b[0m                                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "aml_senders = aml_data[['Sender_Id', 'Sender_Account']].drop_duplicates().\n",
    "fraud_senders = fraud_data[['Sender_Id', 'Sender_Account']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aml_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth Distribution per Year\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 15\u001b[0m plot_hist_df(\u001b[43maml_data\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aml_data' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_hist_df(df):\n",
    "    df['date'] = pd.to_datetime(df['Time_step'])\n",
    "    # Extract the year and month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "\n",
    "    # Plot the distribution of months per year\n",
    "    df.groupby(['year', 'month']).size().unstack().plot(kind='bar', stacked=True)\n",
    "\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Count of Entries')\n",
    "    plt.title('Month Distribution per Year')\n",
    "    plt.show()\n",
    "\n",
    "plot_hist_df(aml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          0\n",
       "          ..\n",
       "1484531    0\n",
       "1484532    0\n",
       "1484533    0\n",
       "1484534    0\n",
       "1484535    0\n",
       "Name: Label, Length: 1484536, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml_data['Label'].map({'GOOD': 0, 'BAD': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['txn_age_days', 'txn_time_hr', 'txn_time_mm'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_norm_values = {\n",
    "    'day': 'txn_age_days',\n",
    "    'hour': 'txn_time_hr',\n",
    "    'min': 'txn_time_mm',\n",
    "}\n",
    "\n",
    "dt_norm_values.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'txn_age_days', 'txn_time_hr', 'txn_time_mm']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['a', 'b'] + list(dt_norm_values.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'txn_age_days'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_norm_values['day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['txn_age_days', 'txn_time_hr', 'txn_time_mm']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dt_norm_values['day'], dt_norm_values['hour'], dt_norm_values['min']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>account_age</th>\n",
       "      <th>volume_7d_avg</th>\n",
       "      <th>volume_7d_std</th>\n",
       "      <th>volume_7d_count</th>\n",
       "      <th>volume_14d_avg</th>\n",
       "      <th>volume_14d_std</th>\n",
       "      <th>volume_14d_count</th>\n",
       "      <th>volume_30d_avg</th>\n",
       "      <th>volume_30d_std</th>\n",
       "      <th>volume_30d_count</th>\n",
       "      <th>volume_avg_7d_to_14d_ratio</th>\n",
       "      <th>volume_count_7d_to_14d_ratio</th>\n",
       "      <th>volume_avg_7d_to_30d_ratio</th>\n",
       "      <th>volume_count_7d_to_30d_ratio</th>\n",
       "      <th>volume_avg_14d_to_30d_ratio</th>\n",
       "      <th>volume_count_14d_to_30d_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1</td>\n",
       "      <td>150</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>70.710678</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2</td>\n",
       "      <td>250</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id  amount  account_age  volume_7d_avg  volume_7d_std  \\\n",
       "0          A1     100           10            0.0            0.0   \n",
       "1          A1     200           20            0.0            0.0   \n",
       "2          A1     150           30            0.0            0.0   \n",
       "3          A2     300           15            0.0            0.0   \n",
       "4          A2     250           25            0.0            0.0   \n",
       "\n",
       "   volume_7d_count  volume_14d_avg  volume_14d_std  volume_14d_count  \\\n",
       "0              0.0             0.0             0.0               0.0   \n",
       "1              0.0           100.0             0.0               1.0   \n",
       "2              0.0           200.0             0.0               1.0   \n",
       "3              0.0             0.0             0.0               0.0   \n",
       "4              0.0           300.0             0.0               1.0   \n",
       "\n",
       "   volume_30d_avg  volume_30d_std  volume_30d_count  \\\n",
       "0             0.0        0.000000               0.0   \n",
       "1           100.0        0.000000               1.0   \n",
       "2           150.0       70.710678               2.0   \n",
       "3             0.0        0.000000               0.0   \n",
       "4           300.0        0.000000               1.0   \n",
       "\n",
       "   volume_avg_7d_to_14d_ratio  volume_count_7d_to_14d_ratio  \\\n",
       "0                         0.0                           0.0   \n",
       "1                         0.0                           0.0   \n",
       "2                         0.0                           0.0   \n",
       "3                         0.0                           0.0   \n",
       "4                         0.0                           0.0   \n",
       "\n",
       "   volume_avg_7d_to_30d_ratio  volume_count_7d_to_30d_ratio  \\\n",
       "0                         0.0                           0.0   \n",
       "1                         0.0                           0.0   \n",
       "2                         0.0                           0.0   \n",
       "3                         0.0                           0.0   \n",
       "4                         0.0                           0.0   \n",
       "\n",
       "   volume_avg_14d_to_30d_ratio  volume_count_14d_to_30d_ratio  \n",
       "0                     0.000000                       0.000000  \n",
       "1                     1.000000                       2.142857  \n",
       "2                     1.333333                       1.071429  \n",
       "3                     0.000000                       0.000000  \n",
       "4                     1.000000                       2.142857  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m calculator \u001b[38;5;241m=\u001b[39m TransactionStatsCalculator(\n\u001b[1;32m      2\u001b[0m         time_bins\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m30\u001b[39m],\n\u001b[1;32m      3\u001b[0m         metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      4\u001b[0m         ratio_pairs\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m14\u001b[39m), (\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m30\u001b[39m), (\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m30\u001b[39m)]\n\u001b[1;32m      5\u001b[0m     )\n\u001b[0;32m----> 7\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[43mcalculator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_transactions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maml_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamount_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUSD_amount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustomer_id_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparty_Id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mage_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtxn_age_days\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m test_data\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/jp_morgan/test_data.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 183\u001b[0m, in \u001b[0;36mTransactionStatsCalculator.process_transactions\u001b[0;34m(self, df, amount_col, customer_id_col, age_col)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Calculate metrics for each transaction\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m--> 183\u001b[0m     stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_transaction_stats\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_transaction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamount_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamount_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustomer_id_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustomer_id_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mage_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mage_col\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;66;03m# Update row with calculated stats\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric, value \u001b[38;5;129;01min\u001b[39;00m stats\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[23], line 122\u001b[0m, in \u001b[0;36mTransactionStatsCalculator.calculate_transaction_stats\u001b[0;34m(self, df, current_transaction, amount_col, customer_id_col, age_col)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calculate all statistics for a single transaction\"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Get customer's transactions up to this point\u001b[39;00m\n\u001b[1;32m    121\u001b[0m customer_df \u001b[38;5;241m=\u001b[39m df[\n\u001b[0;32m--> 122\u001b[0m     (\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcustomer_id_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcurrent_transaction\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcustomer_id_col\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m    123\u001b[0m     (df[age_col] \u001b[38;5;241m<\u001b[39m current_transaction[age_col])\n\u001b[1;32m    124\u001b[0m ]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Calculate stats for each time bin\u001b[39;00m\n\u001b[1;32m    127\u001b[0m all_stats \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Documents/AMIEL/[Personal] Git/.conda/lib/python3.11/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AMIEL/[Personal] Git/.conda/lib/python3.11/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/AMIEL/[Personal] Git/.conda/lib/python3.11/site-packages/pandas/core/series.py:6119\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6116\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   6117\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 6119\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/Documents/AMIEL/[Personal] Git/.conda/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:344\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 344\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/AMIEL/[Personal] Git/.conda/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:129\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    127\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "calculator = TransactionStatsCalculator(\n",
    "        time_bins=[7, 14, 30],\n",
    "        metrics=['avg', 'std', 'count'],\n",
    "        ratio_pairs=[(7, 14), (7, 30), (14, 30)]\n",
    "    )\n",
    "\n",
    "test_data = calculator.process_transactions(\n",
    "    df=aml_data,\n",
    "    amount_col='USD_amount',\n",
    "    customer_id_col='party_Id',\n",
    "    age_col='txn_age_days'\n",
    ")\n",
    "\n",
    "test_data.to_parquet('../data/jp_morgan/test_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/UDAmiel/Documents/AMIEL/[Personal] Git/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2024-11-04 14:09:42.687305\n",
      "Initial memory usage: 885.78 MB\n",
      "Starting processing with 7 workers\n",
      "Total transactions: 1,498,177\n",
      "Total customers: 14,134\n",
      "Processing 15 batches...\n",
      "\n",
      "Processing batch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing customers in batch: 100%|| 1000/1000 [01:20<00:00, 12.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing customers in batch: 100%|| 1000/1000 [01:18<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing customers in batch: 100%|| 1000/1000 [01:36<00:00, 10.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing customers in batch: 100%|| 1000/1000 [01:28<00:00, 11.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing customers in batch: 100%|| 1000/1000 [01:26<00:00, 11.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing customers in batch: 100%|| 1000/1000 [01:16<00:00, 13.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing customers in batch: 100%|| 1000/1000 [01:24<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing customers in batch: 100%|| 1000/1000 [01:30<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing customers in batch: 100%|| 1000/1000 [01:25<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing customers in batch: 100%|| 1000/1000 [01:27<00:00, 11.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing customers in batch: 100%|| 1000/1000 [01:20<00:00, 12.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing customers in batch: 100%|| 1000/1000 [01:24<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing customers in batch: 100%|| 1000/1000 [01:32<00:00, 10.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing customers in batch: 100%|| 1000/1000 [01:27<00:00, 11.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing customers in batch: 100%|| 134/134 [00:10<00:00, 12.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining all results...\n",
      "Calculating ratios...\n",
      "\n",
      "Processing completed successfully!\n",
      "End time: 2024-11-04 14:30:05.937732\n",
      "Total processing time: 0:20:23.250427\n",
      "Final memory usage: 1466.05 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class TransactionProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        time_bins: List[int] = [7, 14, 30],\n",
    "        amount_col: str = 'USD_amount',  # Updated\n",
    "        customer_id_col: str = 'party_Id',  # Updated\n",
    "        age_col: str = 'txn_age_days'  # Updated\n",
    "    ):\n",
    "        self.time_bins = time_bins\n",
    "        self.amount_col = amount_col\n",
    "        self.customer_id_col = customer_id_col\n",
    "        self.age_col = age_col\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_customer_metrics(args: Tuple[pd.DataFrame, Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Calculate metrics for a single customer's transactions\"\"\"\n",
    "        customer_data, config = args\n",
    "        result = customer_data.copy()\n",
    "        \n",
    "        # Get configuration\n",
    "        time_bins = config['time_bins']\n",
    "        amount_col = config['amount_col']\n",
    "        age_col = config['age_col']\n",
    "        \n",
    "        # Sort by age\n",
    "        result = result.sort_values(age_col)\n",
    "        \n",
    "        # Calculate metrics for each time bin\n",
    "        for days in time_bins:\n",
    "            result[f'volume_{days}d_avg'] = 0.0\n",
    "            result[f'volume_{days}d_count'] = 0\n",
    "            \n",
    "            for idx in range(len(result)):\n",
    "                current_age = result.iloc[idx][age_col]\n",
    "                mask = (\n",
    "                    (result[age_col] <= (current_age - 1)) & \n",
    "                    (result[age_col] > (current_age - days - 1))\n",
    "                )\n",
    "                window_data = result.loc[mask, amount_col]\n",
    "                \n",
    "                if len(window_data) > 0:\n",
    "                    result.iloc[idx, result.columns.get_loc(f'volume_{days}d_avg')] = window_data.mean()\n",
    "                    result.iloc[idx, result.columns.get_loc(f'volume_{days}d_count')] = len(window_data)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def process_transactions(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        n_jobs: int = None,\n",
    "        batch_size: int = 1000\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Process all transactions using parallel processing\"\"\"\n",
    "        if n_jobs is None:\n",
    "            n_jobs = max(1, mp.cpu_count() - 1)\n",
    "        \n",
    "        print(f\"Starting processing with {n_jobs} workers\")\n",
    "        print(f\"Total transactions: {len(df):,}\")\n",
    "        print(f\"Total customers: {df[self.customer_id_col].nunique():,}\")\n",
    "        \n",
    "        # Sort data\n",
    "        df = df.sort_values([self.customer_id_col, self.age_col])\n",
    "        \n",
    "        # Split customers into batches\n",
    "        all_customers = df[self.customer_id_col].unique()\n",
    "        customer_batches = [\n",
    "            all_customers[i:i + batch_size] \n",
    "            for i in range(0, len(all_customers), batch_size)\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        config = {\n",
    "            'time_bins': self.time_bins,\n",
    "            'amount_col': self.amount_col,\n",
    "            'age_col': self.age_col\n",
    "        }\n",
    "        \n",
    "        print(f\"Processing {len(customer_batches)} batches...\")\n",
    "        \n",
    "        for batch_idx, customer_batch in enumerate(customer_batches, 1):\n",
    "            print(f\"\\nProcessing batch {batch_idx}/{len(customer_batches)}\")\n",
    "            \n",
    "            # Get data for current batch\n",
    "            batch_data = df[df[self.customer_id_col].isin(customer_batch)]\n",
    "            \n",
    "            # Process this batch without multiprocessing\n",
    "            batch_results = []\n",
    "            for _, group in tqdm(batch_data.groupby(self.customer_id_col), \n",
    "                               desc=\"Processing customers in batch\"):\n",
    "                result = self.calculate_customer_metrics((group, config))\n",
    "                batch_results.append(result)\n",
    "            \n",
    "            # Combine batch results\n",
    "            if batch_results:\n",
    "                batch_df = pd.concat(batch_results, ignore_index=True)\n",
    "                results.append(batch_df)\n",
    "            \n",
    "            # Save intermediate results\n",
    "            if batch_idx % 5 == 0:  # Save every 5 batches\n",
    "                interim_df = pd.concat(results, ignore_index=True)\n",
    "                interim_df.to_parquet(f'interim_results_batch_{batch_idx}.parquet')\n",
    "            \n",
    "            # Clear memory\n",
    "            del batch_data, batch_results\n",
    "            gc.collect()\n",
    "        \n",
    "        # Combine all results\n",
    "        print(\"\\nCombining all results...\")\n",
    "        result_df = pd.concat(results, ignore_index=True)\n",
    "        \n",
    "        # Calculate ratios\n",
    "        print(\"Calculating ratios...\")\n",
    "        for days1, days2 in [(7, 14), (7, 30), (14, 30)]:\n",
    "            # Average ratio\n",
    "            result_df[f'volume_avg_{days1}d_to_{days2}d_ratio'] = (\n",
    "                result_df[f'volume_{days1}d_avg'] / \n",
    "                result_df[f'volume_{days2}d_avg'].replace(0, np.nan)\n",
    "            ).fillna(0)\n",
    "            \n",
    "            # Count ratio\n",
    "            result_df[f'volume_count_{days1}d_to_{days2}d_ratio'] = (\n",
    "                (result_df[f'volume_{days1}d_count'] / days1) /\n",
    "                (result_df[f'volume_{days2}d_count'] / days2).replace(0, np.nan)\n",
    "            ).fillna(0)\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "def process_with_monitoring(\n",
    "    df: pd.DataFrame,\n",
    "    time_bins: List[int] = [7, 14, 30],\n",
    "    batch_size: int = 1000\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Process with monitoring\"\"\"\n",
    "    import psutil\n",
    "    import os\n",
    "    \n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    print(f\"Start time: {start_time}\")\n",
    "    print(f\"Initial memory usage: {process.memory_info().rss / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize processor\n",
    "        processor = TransactionProcessor(\n",
    "            time_bins=time_bins,\n",
    "            amount_col='USD_amount',\n",
    "            customer_id_col='party_Id',\n",
    "            age_col='txn_age_days'\n",
    "        )\n",
    "        \n",
    "        # Process data\n",
    "        result_df = processor.process_transactions(\n",
    "            df=df,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        print(\"\\nProcessing completed successfully!\")\n",
    "        print(f\"End time: {end_time}\")\n",
    "        print(f\"Total processing time: {processing_time}\")\n",
    "        print(f\"Final memory usage: {process.memory_info().rss / 1024 / 1024:.2f} MB\")\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during processing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Read your data\n",
    "    df = aml_data.copy()  # or however you load your data\n",
    "    \n",
    "    # Process with monitoring\n",
    "    result_df = process_with_monitoring(\n",
    "        df=df,\n",
    "        time_bins=[7, 14, 30],\n",
    "        batch_size=1000  # Process 1000 customers at a time\n",
    "    )\n",
    "    \n",
    "    # Save final results\n",
    "    result_df.to_parquet('transaction_metrics_final_fraud.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pl = pl.read_parquet('transaction_metrics_final.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
