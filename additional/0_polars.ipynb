{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "data_dir = '../data/jp_morgan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(f'{data_dir}/transaction_metrics_final_aml.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2027-12-04 13:20:00'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Time_step'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_velocity(\n",
    "        df: pd.DataFrame, \n",
    "        pattern: str = \"volume_{day}d_{metric}\",\n",
    "        days: List[int] = [7, 14, 30]\n",
    "        ) -> pd.DataFrame:\n",
    "    \n",
    "    for d in days:\n",
    "        avg_col = pattern.format(day=d, metric='avg')\n",
    "        count_col = pattern.format(day=d, metric='count')\n",
    "        velocity_col = f'stat_{d}d_velocity'\n",
    "        df = df.with_columns((df[avg_col] * df[count_col] / d).alias(velocity_col))\n",
    "\n",
    "    return df\n",
    "\n",
    "test_data = count_velocity(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expr.rolling() missing 1 required positional argument: 'index_column'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 381\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m analyzed_df, results\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 381\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 377\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    373\u001b[0m eda \u001b[38;5;241m=\u001b[39m FraudEDA(column_config\u001b[38;5;241m=\u001b[39mcustom_columns)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# Load your data    \u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# Run the full EDA\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m analyzed_df, results \u001b[38;5;241m=\u001b[39m \u001b[43meda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_full_eda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m analyzed_df, results\n",
      "Cell \u001b[0;32mIn[4], line 337\u001b[0m, in \u001b[0;36mFraudEDA.run_full_eda\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    334\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_and_clean_data(df)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# Add risk features\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_risk_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# Run all analyses\u001b[39;00m\n\u001b[1;32m    340\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[4], line 319\u001b[0m, in \u001b[0;36mFraudEDA.add_risk_features\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_risk_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, df: pl\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pl\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m    307\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m    Add fraud detection specific features\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mwith_columns([\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;66;03m# Calculate amount z-score per transaction type\u001b[39;00m\n\u001b[1;32m    312\u001b[0m         (pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mamount_col) \u001b[38;5;241m-\u001b[39m pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mamount_col)\u001b[38;5;241m.\u001b[39mmean()) \u001b[38;5;241m/\u001b[39m \n\u001b[1;32m    313\u001b[0m         pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mamount_col)\u001b[38;5;241m.\u001b[39mstd()\n\u001b[1;32m    314\u001b[0m             \u001b[38;5;241m.\u001b[39mover(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtransaction_type_col)\n\u001b[1;32m    315\u001b[0m             \u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamount_zscore\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    316\u001b[0m         \n\u001b[1;32m    317\u001b[0m         \u001b[38;5;66;03m# Calculate transaction velocity (number of transactions in last hour)\u001b[39;00m\n\u001b[1;32m    318\u001b[0m         \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate_parsed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 319\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrolling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperiod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1h\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m             \u001b[38;5;241m.\u001b[39mover(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39maccount_col)\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtx_velocity_1h\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    322\u001b[0m         \n\u001b[1;32m    323\u001b[0m         \u001b[38;5;66;03m# Flag structured transactions (just below common reporting thresholds)\u001b[39;00m\n\u001b[1;32m    324\u001b[0m         (pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mamount_col)\u001b[38;5;241m.\u001b[39mis_between(\u001b[38;5;241m9000\u001b[39m, \u001b[38;5;241m10000\u001b[39m) \u001b[38;5;241m|\u001b[39m \n\u001b[1;32m    325\u001b[0m          pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mamount_col)\u001b[38;5;241m.\u001b[39mis_between(\u001b[38;5;241m4500\u001b[39m, \u001b[38;5;241m5000\u001b[39m))\n\u001b[1;32m    326\u001b[0m             \u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_structured\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    327\u001b[0m     ])\n",
      "\u001b[0;31mTypeError\u001b[0m: Expr.rolling() missing 1 required positional argument: 'index_column'"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "\n",
    "@dataclass\n",
    "class ColumnConfig:\n",
    "    \"\"\"Configuration for column names in the dataset\"\"\"\n",
    "    date_col: str\n",
    "    transaction_type_col: str\n",
    "    transaction_method_col: str\n",
    "    amount_col: str\n",
    "    party_col: str\n",
    "    counterparty_col: str\n",
    "    account_col: str\n",
    "    label_col: str  # The column indicating fraud/non-fraud\n",
    "    positive_label: any  # Value indicating legitimate transaction in label column\n",
    "    \n",
    "    @classmethod\n",
    "    def default(cls) -> 'ColumnConfig':\n",
    "        \"\"\"Default column configuration\"\"\"\n",
    "        return cls(\n",
    "            date_col='date',\n",
    "            transaction_type_col='transaction_type',\n",
    "            transaction_method_col='transaction_method',\n",
    "            amount_col='amount',\n",
    "            party_col='party',\n",
    "            counterparty_col='cparty',\n",
    "            account_col='account_identifier',\n",
    "            label_col='is_good',\n",
    "            positive_label=True\n",
    "        )\n",
    "\n",
    "class FraudEDA:\n",
    "    \"\"\"Fraud Detection Exploratory Data Analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, column_config: Optional[ColumnConfig] = None):\n",
    "        self.config = column_config or ColumnConfig.default()\n",
    "    \n",
    "    def load_and_clean_data(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Initial data loading and cleaning using Polars\n",
    "        \"\"\"\n",
    "        # Convert date column to datetime and extract temporal features\n",
    "        df = df.with_columns([\n",
    "            pl.col(self.config.date_col).str.to_datetime().alias('date_parsed'),\n",
    "            pl.col(self.config.date_col).str.to_datetime().dt.hour().alias('hour'),\n",
    "            pl.col(self.config.date_col).str.to_datetime().dt.weekday().alias('day_of_week'),\n",
    "            pl.col(self.config.date_col).str.to_datetime().dt.month().alias('month')\n",
    "        ])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def basic_statistics(self, df: pl.DataFrame) -> Dict[str, pl.DataFrame]:\n",
    "        \"\"\"\n",
    "        Calculate basic statistics for numerical columns\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Basic Statistics ===\")\n",
    "        \n",
    "        # Numerical summary for amount\n",
    "        amount_stats = df.select([\n",
    "            pl.col(self.config.amount_col).mean().alias('mean'),\n",
    "            pl.col(self.config.amount_col).median().alias('median'),\n",
    "            pl.col(self.config.amount_col).std().alias('std'),\n",
    "            pl.col(self.config.amount_col).min().alias('min'),\n",
    "            pl.col(self.config.amount_col).max().alias('max'),\n",
    "            pl.col(self.config.amount_col).quantile(0.25).alias('25%'),\n",
    "            pl.col(self.config.amount_col).quantile(0.75).alias('75%')\n",
    "        ])\n",
    "        print(\"\\nAmount Statistics:\")\n",
    "        print(amount_stats)\n",
    "        \n",
    "        # Missing values\n",
    "        null_counts = df.null_count()\n",
    "        print(\"\\nMissing Values:\")\n",
    "        print(null_counts)\n",
    "        \n",
    "        # Class distribution\n",
    "        class_dist = (df\n",
    "            .select(pl.col(self.config.label_col))\n",
    "            .groupby(self.config.label_col)\n",
    "            .count()\n",
    "            .with_columns(\n",
    "                (pl.col('count') / pl.col('count').sum()).alias('percentage')\n",
    "            )\n",
    "        )\n",
    "        print(\"\\nClass Distribution:\")\n",
    "        print(class_dist)\n",
    "        \n",
    "        return {\n",
    "            'amount_stats': amount_stats,\n",
    "            'null_counts': null_counts,\n",
    "            'class_dist': class_dist\n",
    "        }\n",
    "    \n",
    "    def temporal_analysis(self, df: pl.DataFrame) -> Dict[str, pl.DataFrame]:\n",
    "        \"\"\"\n",
    "        Analyze temporal patterns\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Temporal Patterns ===\")\n",
    "        \n",
    "        # Transaction volume by hour\n",
    "        hourly_stats = (df\n",
    "            .groupby(['hour', self.config.label_col])\n",
    "            .count()\n",
    "            .pivot(\n",
    "                values='count',\n",
    "                index='hour',\n",
    "                columns=self.config.label_col\n",
    "            )\n",
    "            .sort('hour')\n",
    "        )\n",
    "        print(\"\\nHourly Transaction Patterns:\")\n",
    "        print(hourly_stats)\n",
    "        \n",
    "        # Transaction volume by day of week\n",
    "        daily_stats = (df\n",
    "            .groupby(['day_of_week', self.config.label_col])\n",
    "            .count()\n",
    "            .pivot(\n",
    "                values='count',\n",
    "                index='day_of_week',\n",
    "                columns=self.config.label_col\n",
    "            )\n",
    "            .sort('day_of_week')\n",
    "        )\n",
    "        print(\"\\nDaily Transaction Patterns:\")\n",
    "        print(daily_stats)\n",
    "        \n",
    "        return {\n",
    "            'hourly_stats': hourly_stats,\n",
    "            'daily_stats': daily_stats\n",
    "        }\n",
    "    \n",
    "    def amount_distribution_analysis(self, df: pl.DataFrame) -> Dict[str, pl.DataFrame]:\n",
    "        \"\"\"\n",
    "        Analyze transaction amount patterns\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Amount Analysis ===\")\n",
    "        \n",
    "        # Amount statistics by transaction type\n",
    "        amount_by_type = (df\n",
    "            .groupby(self.config.transaction_type_col)\n",
    "            .agg([\n",
    "                pl.col(self.config.amount_col).mean().alias('mean'),\n",
    "                pl.col(self.config.amount_col).median().alias('median'),\n",
    "                pl.col(self.config.amount_col).std().alias('std')\n",
    "            ])\n",
    "        )\n",
    "        print(\"\\nAmount Statistics by Transaction Type:\")\n",
    "        print(amount_by_type)\n",
    "        \n",
    "        # Calculate amount percentiles for good vs fraudulent transactions\n",
    "        percentiles = [0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "        amount_percentiles = (df\n",
    "            .groupby(self.config.label_col)\n",
    "            .agg([\n",
    "                pl.col(self.config.amount_col).quantile(p).alias(f'p{int(p*100)}')\n",
    "                for p in percentiles\n",
    "            ])\n",
    "        )\n",
    "        print(\"\\nAmount Percentiles by Class:\")\n",
    "        print(amount_percentiles)\n",
    "        \n",
    "        return {\n",
    "            'amount_by_type': amount_by_type,\n",
    "            'amount_percentiles': amount_percentiles\n",
    "        }\n",
    "    \n",
    "    def behavioral_analysis(self, df: pl.DataFrame) -> Dict[str, pl.DataFrame]:\n",
    "        \"\"\"\n",
    "        Analyze behavioral patterns\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Behavioral Analysis ===\")\n",
    "        \n",
    "        # Transaction method preferences\n",
    "        method_stats = (df\n",
    "            .groupby([self.config.transaction_method_col, self.config.label_col])\n",
    "            .count()\n",
    "            .pivot(\n",
    "                values='count',\n",
    "                index=self.config.transaction_method_col,\n",
    "                columns=self.config.label_col\n",
    "            )\n",
    "        )\n",
    "        print(\"\\nTransaction Method Distribution:\")\n",
    "        print(method_stats)\n",
    "        \n",
    "        # Account activity patterns\n",
    "        account_activity = (df\n",
    "            .groupby(self.config.account_col)\n",
    "            .agg([\n",
    "                pl.col(self.config.amount_col).count().alias('transaction_count'),\n",
    "                pl.col(self.config.amount_col).sum().alias('total_amount'),\n",
    "                pl.col(self.config.amount_col).mean().alias('avg_amount'),\n",
    "                pl.col(self.config.amount_col).std().alias('std_amount'),\n",
    "                pl.col(self.config.label_col).mean().alias('good_ratio')\n",
    "            ])\n",
    "        )\n",
    "        print(\"\\nAccount Activity Patterns:\")\n",
    "        print(account_activity.head())\n",
    "        \n",
    "        return {\n",
    "            'method_stats': method_stats,\n",
    "            'account_activity': account_activity\n",
    "        }\n",
    "    \n",
    "    def party_analysis(self, df: pl.DataFrame) -> Dict[str, pl.DataFrame]:\n",
    "        \"\"\"\n",
    "        Analyze party/counterparty patterns\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Party Analysis ===\")\n",
    "        \n",
    "        # Party transaction patterns\n",
    "        party_stats = (df\n",
    "            .groupby(self.config.party_col)\n",
    "            .agg([\n",
    "                pl.col(self.config.amount_col).count().alias('transaction_count'),\n",
    "                pl.col(self.config.amount_col).sum().alias('total_amount'),\n",
    "                pl.col(self.config.amount_col).mean().alias('avg_amount'),\n",
    "                pl.col(self.config.label_col).mean().alias('good_ratio')\n",
    "            ])\n",
    "            .sort('transaction_count', reverse=True)\n",
    "        )\n",
    "        print(\"\\nParty Transaction Patterns:\")\n",
    "        print(party_stats.head())\n",
    "        \n",
    "        # Analyze party-counterparty relationships\n",
    "        party_cparty_freq = (df\n",
    "            .groupby([self.config.party_col, self.config.counterparty_col])\n",
    "            .agg([\n",
    "                pl.col(self.config.amount_col).count().alias('transaction_count')\n",
    "            ])\n",
    "            .sort('transaction_count', reverse=True)\n",
    "        )\n",
    "        print(\"\\nTop Party-Counterparty Relationships:\")\n",
    "        print(party_cparty_freq.head())\n",
    "        \n",
    "        return {\n",
    "            'party_stats': party_stats,\n",
    "            'party_cparty_freq': party_cparty_freq\n",
    "        }\n",
    "    \n",
    "    def velocity_checks(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform velocity checks\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Velocity Checks ===\")\n",
    "        \n",
    "        # Add hour window\n",
    "        df = df.with_columns([\n",
    "            pl.col('date_parsed').dt.truncate('1h').alias('hour_window')\n",
    "        ])\n",
    "        \n",
    "        # Transaction velocity per account (1-hour window)\n",
    "        velocity_1h = (df\n",
    "            .groupby([self.config.account_col, 'hour_window'])\n",
    "            .agg([\n",
    "                pl.col(self.config.amount_col).count().alias('transaction_count'),\n",
    "                pl.col(self.config.amount_col).sum().alias('total_amount'),\n",
    "                pl.col(self.config.label_col).mean().alias('good_ratio')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        # Calculate velocity statistics\n",
    "        velocity_stats = velocity_1h.describe()\n",
    "        print(\"\\nHourly Velocity Statistics:\")\n",
    "        print(velocity_stats)\n",
    "        \n",
    "        return velocity_1h\n",
    "    \n",
    "    def generate_visualizations(self, df: pl.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Generate key visualizations\n",
    "        \"\"\"\n",
    "        plt.style.use('seaborn')\n",
    "        \n",
    "        # Convert to pandas for visualization (as seaborn works better with pandas)\n",
    "        df_pd = df.to_pandas()\n",
    "        \n",
    "        # Amount distribution by class\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x=self.config.label_col, y=self.config.amount_col, data=df_pd)\n",
    "        plt.title('Transaction Amount Distribution by Class')\n",
    "        plt.yscale('log')\n",
    "        \n",
    "        # Temporal heatmap\n",
    "        temporal_heatmap = (df\n",
    "            .groupby(['day_of_week', 'hour'])\n",
    "            .count()\n",
    "            .pivot(\n",
    "                values='count',\n",
    "                index='day_of_week',\n",
    "                columns='hour'\n",
    "            )\n",
    "            .to_pandas()\n",
    "        )\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.heatmap(temporal_heatmap, cmap='YlOrRd')\n",
    "        plt.title('Transaction Volume Heatmap')\n",
    "    \n",
    "    def add_risk_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Add fraud detection specific features\n",
    "        \"\"\"\n",
    "        return df.with_columns([\n",
    "            # Calculate amount z-score per transaction type\n",
    "            (pl.col(self.config.amount_col) - pl.col(self.config.amount_col).mean()) / \n",
    "            pl.col(self.config.amount_col).std()\n",
    "                .over(self.config.transaction_type_col)\n",
    "                .alias('amount_zscore'),\n",
    "            \n",
    "            # Calculate transaction velocity (number of transactions in last hour)\n",
    "            pl.col('date_parsed')\n",
    "                .rolling(period='1h')\n",
    "                .over(self.config.account_col)\n",
    "                .alias('tx_velocity_1h'),\n",
    "            \n",
    "            # Flag structured transactions (just below common reporting thresholds)\n",
    "            (pl.col(self.config.amount_col).is_between(9000, 10000) | \n",
    "             pl.col(self.config.amount_col).is_between(4500, 5000))\n",
    "                .alias('is_structured')\n",
    "        ])\n",
    "    \n",
    "    def run_full_eda(self, df: pl.DataFrame) -> Tuple[pl.DataFrame, Dict[str, pl.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Run complete EDA pipeline\n",
    "        \"\"\"\n",
    "        # Clean and prepare data\n",
    "        df = self.load_and_clean_data(df)\n",
    "        \n",
    "        # Add risk features\n",
    "        df = self.add_risk_features(df)\n",
    "        \n",
    "        # Run all analyses\n",
    "        results = {}\n",
    "        results.update(self.basic_statistics(df))\n",
    "        results.update(self.temporal_analysis(df))\n",
    "        results.update(self.amount_distribution_analysis(df))\n",
    "        results.update(self.behavioral_analysis(df))\n",
    "        results.update(self.party_analysis(df))\n",
    "        \n",
    "        # Generate visualizations\n",
    "        self.generate_visualizations(df)\n",
    "        \n",
    "        # Additional fraud-specific metrics\n",
    "        fraud_rate = (1 - df.select(pl.col(self.config.label_col).mean()).item()) * 100\n",
    "        print(\"\\n=== Fraud Detection Metrics ===\")\n",
    "        print(f\"Fraud Rate: {fraud_rate:.2f}%\")\n",
    "        \n",
    "        return df, results\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Define custom column names\n",
    "    custom_columns = ColumnConfig(\n",
    "        date_col='Time_step',\n",
    "        transaction_type_col='std_txn_type',\n",
    "        transaction_method_col='std_txn_method',\n",
    "        amount_col='USD_amount',\n",
    "        party_col='party_Id',\n",
    "        counterparty_col='cparty_Id',\n",
    "        account_col='party_Account',\n",
    "        label_col='Label',\n",
    "        positive_label='GOOD'\n",
    "    )\n",
    "    \n",
    "    # Initialize the EDA class with custom column configuration\n",
    "    eda = FraudEDA(column_config=custom_columns)\n",
    "    \n",
    "    # Load your data    \n",
    "    # Run the full EDA\n",
    "    analyzed_df, results = eda.run_full_eda(df)\n",
    "    return analyzed_df, results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.with_columns((test_data['cparty_Id'].str.contains('JPMC')).alias('cparty_internal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_data\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.sort(by=['Time_step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Transaction_Type</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;PAYMENT&quot;</td></tr><tr><td>&quot;CASH-DEPOSIT&quot;</td></tr><tr><td>&quot;WIRE&quot;</td></tr><tr><td>&quot;CRYPTO-TRANSFER&quot;</td></tr><tr><td>&quot;WITHDRAWL&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5,)\n",
       "Series: 'Transaction_Type' [str]\n",
       "[\n",
       "\t\"PAYMENT\"\n",
       "\t\"CASH-DEPOSIT\"\n",
       "\t\"WIRE\"\n",
       "\t\"CRYPTO-TRANSFER\"\n",
       "\t\"WITHDRAWL\"\n",
       "]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.get_column('Transaction_Type').unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_mapping = {\n",
    "  \"CASH-DEPOSIT\": {\n",
    "    \"std_txn_type\": \"DEPOSIT\",\n",
    "    \"std_txn_method\": \"CASH\"\n",
    "  },\n",
    "  \"WITHDRAWL\": {\n",
    "    \"std_txn_type\": \"WITHDRAWAL\",\n",
    "    \"std_txn_method\": \"CASH\"\n",
    "  },\n",
    "  \"WIRE\": {\n",
    "    \"std_txn_type\": \"TRANSFER\",\n",
    "    \"std_txn_method\": \"ELECTRONIC\"\n",
    "  },\n",
    "  \"PAYMENT\": {\n",
    "    \"std_txn_type\": \"PAYMENT\",\n",
    "    \"std_txn_method\": \"ELECTRONIC\"\n",
    "  },\n",
    "  \"CRYPTO-TRANSFER\": {\n",
    "    \"std_txn_type\": \"TRANSFER\",\n",
    "    \"std_txn_method\": \"ELECTRONIC\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_map_transactions(df: pl.DataFrame, txn_mapping: dict) -> pl.DataFrame:\n",
    "    # Pre-compute the mappings\n",
    "    type_mapping = {k: v['std_txn_type'] for k, v in txn_mapping.items()}\n",
    "    method_mapping = {k: v['std_txn_method'] for k, v in txn_mapping.items()}\n",
    "    \n",
    "    # First check for unmapped values\n",
    "    unique_txn_types = df.get_column('Transaction_Type').unique().to_list()  # Note the column name change\n",
    "    unmapped_txns = [txn for txn in unique_txn_types if txn not in type_mapping]\n",
    "    \n",
    "    if unmapped_txns:\n",
    "        raise ValueError(\n",
    "            f\"Found {len(unmapped_txns)} unmapped transaction types:\\n\"\n",
    "            f\"{unmapped_txns}\\n\"\n",
    "            f\"Please update the mapping dictionary with these values.\"\n",
    "        )\n",
    "    \n",
    "    # If we get here, all values are mapped, so proceed with mapping\n",
    "    return df.with_columns([\n",
    "        pl.col('Transaction_Type').replace(type_mapping).alias('std_txn_type'),\n",
    "        pl.col('Transaction_Type').replace(method_mapping).alias('std_txn_method')\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = map_transactions(df, txn_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
